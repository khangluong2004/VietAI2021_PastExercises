{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_DeepNeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAY-HQLVVK05"
      },
      "source": [
        "## Assignment 2: Deep Neural Network \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fciXmk4mVK06"
      },
      "source": [
        "### Giới thiệu\n",
        "\n",
        "Để có thể hoàn tất bài tập này, các bạn cần nắm rõ những kiến thức sau:\n",
        "\n",
        "    - Neural Networks - Fully connected networks là gì, nguyên tắc hoạt động ra sao.\n",
        "\t- Giải thuật Feedforward và BackPropagation trong bài toán NN.\n",
        "\t- Giải thuật gradient descent - Batch and Mini-batch.\n",
        "\t- Regularization để tránh overfitting trong NN.\n",
        "\n",
        "Các bạn có thể tham khảo lại bài giảng của lớp để nắm vững các nội dung này. Ngoài ra, các bạn có thể đặt câu hỏi cho đội ngũ giảng dạy nếu có thắc mắc. \n",
        "\n",
        "Trong bài tập này các bạn sẽ sử dụng Neural Networks để giải quyết 2 bài toán:\n",
        "\n",
        "\t- Bài 1: phân loại dữ liệu BAT, gồm 3 lớp.\n",
        "![Dữ liệu 3 class BAT](https://i.imgur.com/d1Pd1XT.png)\n",
        "\t- Bài 2: phân loại tập fashion MNIST, gồm 10 lớp.\n",
        "![Dữ liệu Fashion MNIST](https://i.imgur.com/O9dqdId.png)\n",
        "Yêu cầu dành cho các bạn trong là giải quyết hai bài trên bằng Numpy và TensorFlow.\n",
        "\n",
        "Mục tiêu của bài tập lần này là hiện thực Neural Networks mạng Fully Connected một cách cơ bản trên Numpy và Tensorflow. Một mạng cơ bản sẽ gồm nhiều hidden layers và một lớp softmax tại layer cuối cùng phù hợp cho việc phân loại dữ liệu. \n",
        "\n",
        "![Mạng neural network. Nguồn: graphicsminer.com/neuralnetwork](https://i.imgur.com/K3Yvt20.png)\n",
        "\n",
        "Khi thiết kế một mạng cơ bản thì người dùng có thể quyết định số input feature cho tầng input. Số output sẽ là số lớp mà người đó muốn phân loại. Ví dụ như bài toán fashion MNIST thì số feature đầu vào chính bằng số pixel của mỗi ảnh, số nút đầu ra sẽ bằng số lớp cần phân loại (10). Đối với số lượng hidden layer và số lượng nodes tương ứng, ta có thể tùy chọn.\n",
        "\n",
        "Một chú ý rất quan trọng là số nodes đầu ra của layer trước sẽ là số inputs đầu vào của layers sau đó.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPH8uEJCVK07"
      },
      "source": [
        "## I. Thực hiện Deep Neural Network với Numpy\n",
        "### Những công việc bạn phải thực hiện \n",
        "\n",
        "1. Các hàm activation `sigmoid`, `tanh`, `relu`, `softmax` và đạo hàm của nó `sigmoid_grad`, `tanh_grad`, `relu_grad`.\n",
        "2. Hàm `forward` và `backward` ở class `HiddenLayer`\n",
        "```python\n",
        "    class HiddenLayer:\n",
        "    \n",
        "        def forward(self, X):\n",
        "            ...\n",
        "    \n",
        "        def backward(self, X, delta_prev):\n",
        "            ...\n",
        "```\n",
        "3. Hàm `forward`, `backward`, `compute_loss` ở class `NeuralNetwork`\n",
        "```python\n",
        "    class NeuralNetwork:\n",
        "        \n",
        "        def forward(self, X):\n",
        "            ...\n",
        "            \n",
        "        def backward(self, X, Y, layers):\n",
        "            ...\n",
        "            \n",
        "        def compute_loss(self, Y, Y_hat):\n",
        "            ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TohoN6f1VK07"
      },
      "source": [
        "### Ký hiệu\n",
        "\n",
        "- $L$: số layers trong mạng neural network. \n",
        "- $l = 0,1,..,L$ với $0$ là layer input và $L$ layer output.\n",
        "- $n^{[l]}$ là số neurons tại layer $l$\n",
        "- $l-1$: layer trước theo chiều forward của $l$.\n",
        "- $l+1$: layer trước theo chiều backward của $l$.\n",
        "- $\\sigma'(x)$: đạo hàm hàm activation theo x (general case cho cả đạo hàm sigmoid, tanh, relu).\n",
        "- $Z^{[l]}$: linear function values tại layer $l$.\n",
        "- $A^{[l]}$: activation function values tại layer $l$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dneL9wNuVK08"
      },
      "source": [
        "### Import các thư viện cần thiết"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He9JhemNND3D"
      },
      "source": [
        "**Chú ý:** Nếu bạn chạy trên Google Colab thì các thư viện này đã được tích hợp sẵn. Nếu bạn chạy trên máy cá nhân, bạn cần install các thư viện *numpy*, *matplotlib*, *googledrivedownloader*, *sklearn*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5pqVHsNVK08"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "import os\n",
        "import sys\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGdoufr7VK0-"
      },
      "source": [
        "### Download dữ liệu và các utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GG7Zj3aVK0_",
        "outputId": "240b5e2d-588a-4aae-eb8b-ae831346daf8"
      },
      "source": [
        "# DOWNLOAD DATA\n",
        "gdd.download_file_from_google_drive(file_id='1EXSdvCLlcXvl1Gi6sNSJCu6psICoLNNc', \n",
        "                                    dest_path=os.path.join(os.getcwd(), 'Assignment2.zip'), unzip=True)\n",
        "\n",
        "if sys.platform.startswith(\"win\"):\n",
        "    !move \"./Assignment2/data\" \".\"\n",
        "    !move \"./Assignment2/test\" \".\"A\n",
        "    !move \"./Assignment2/utils\" \".\"\n",
        "    !del \"Assignment2.zip\"\n",
        "    !rd /s /q \"Assignment2\" \"__MACOSX\"\n",
        "    !dir\n",
        "else:\n",
        "    !mv Assignment2/* .\n",
        "    !rm Assignment2.zip\n",
        "    !rm -rf Assignment2 __MACOSX\n",
        "    # SHOW THE ITEMS OF CURRENT DIRRECTORY\n",
        "    !ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1EXSdvCLlcXvl1Gi6sNSJCu6psICoLNNc into /content/Assignment2.zip... Done.\n",
            "Unzipping...Done.\n",
            "data  sample_data  test  utils\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsUETR3AVixy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctdckAhpYkqf"
      },
      "source": [
        "### Import utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJeV7fvvYtl1"
      },
      "source": [
        "from utils.util import *\n",
        "from utils.gradient_check import *"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVLd-jBpVK1B"
      },
      "source": [
        "### Các hàm activation\n",
        "\n",
        "$$sigmoid(x) = \\frac{1}{1+e^{-x}}$$ \n",
        "\n",
        "$$tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$\n",
        "\n",
        "$$relu(x) = \\begin{cases} x, & \\mbox{if } x > 0 \\\\ 0, & \\mbox{if } x <= 0 \\end{cases}$$\n",
        "\n",
        "$$sigmoid'(x) = sigmoid(x)(1 - sigmoid(x)) $$\n",
        "\n",
        "$$tanh'(x) = 1 - tanh^2(x)$$\n",
        "\n",
        "$$relu'(x) = \\begin{cases} 1, & \\mbox{if } x > 0 \\\\ 0, & \\mbox{if } x <= 0 \\end{cases}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQxzv26Lo1a_"
      },
      "source": [
        "#### \\[TODO 1\\] Cài đặt các hàm activation\n",
        "Định nghĩa các hàm activation ở cell bên dưới. (1đ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI_pLSHVVK1C"
      },
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Sigmoid function.\n",
        "    :param x: input\n",
        "    \"\"\"\n",
        "    #### [TODO 1] START CODE HERE #### \n",
        "    result = 1/ (1 + np.exp(-1 * x.copy()))\n",
        "    #### END CODE HERE ####\n",
        "    return result\n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    \"\"\"\n",
        "    Compute gradient of sigmoid.\n",
        "    :param x: input\n",
        "    \"\"\"\n",
        "    \n",
        "    #### [TODO 1] START CODE HERE #### \n",
        "    da = sigmoid(x) * (1 - sigmoid(x.copy()))\n",
        "    #### END CODE HERE ####\n",
        "    return da\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    Rectified linear unit function.\n",
        "    :param x: input\n",
        "    \"\"\"\n",
        "    \n",
        "    #### [TODO 1] START CODE HERE #### \n",
        "    result = x.copy()\n",
        "    result[(result <= 0)] = 0\n",
        "    #### END CODE HERE ####\n",
        "    return result\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "    \"\"\"\n",
        "    Compute gradient of ReLU.\n",
        "    :param x: input\n",
        "    \"\"\"\n",
        "    \n",
        "    #### [TODO 1] START CODE HERE #### \n",
        "    da = x.copy()\n",
        "    da[da > 0] = 1\n",
        "    da[da <= 0] = 0\n",
        "    #### END CODE HERE ####\n",
        "    return da\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    \"\"\"\n",
        "    Tanh function.\n",
        "    :param x: input\n",
        "    \"\"\"\n",
        "   \n",
        "    #### [TODO 1] START CODE HERE #### \n",
        "    x = x.copy()\n",
        "    result = (np.exp(x) - np.exp(-1 * x))/ (np.exp(x) + np.exp(-1 * x))\n",
        "    #### END CODE HERE ####\n",
        "    return result\n",
        "\n",
        "\n",
        "def tanh_grad(x):\n",
        "    \"\"\"\n",
        "    Compute gradient for tanh.\n",
        "    :param x: input\n",
        "    \"\"\"\n",
        "\n",
        "    #### [TODO 1] START CODE HERE ####\n",
        "    da =  1 - np.square(tanh(x))\n",
        "    #### END CODE HERE ####\n",
        "    return da\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Stable softmax function.\n",
        "    :param x: input\n",
        "    \"\"\"\n",
        "    #### [TODO 1] START CODE HERE ####\n",
        "    z = x.copy()\n",
        "    s = np.atleast_2d(np.sum(np.exp(z), axis = 1)).T\n",
        "    probs = np.exp(z)/ s\n",
        "    #### END CODE HERE ####\n",
        "    return (probs) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c92wODTVK1E"
      },
      "source": [
        "#### Kiểm tra lại lại các hàm activation đã cài đặt\n",
        "\n",
        "Bạn có thể kiểm tra cái hàm bạn đã cài đặt bằng đoạn code bên dưới."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Neen94acVK1E",
        "outputId": "a7e6a334-aa76-4d3d-b0d9-d436e443a5c4"
      },
      "source": [
        "import pickle\n",
        "\n",
        "np.random.seed(2019)\n",
        "func_test = [\"sigmoid\", \"relu\", \"tanh\", \"sigmoid_grad\", \"relu_grad\", \"tanh_grad\", \"softmax\"]\n",
        "test_activation = dict()\n",
        "results = []\n",
        "with open(\"test/activation.pkl\", \"rb\") as f:\n",
        "    test_activation = pickle.load(f)\n",
        "\n",
        "test_x = test_activation[\"test_x\"]\n",
        "sample = test_activation[\"sample\"]\n",
        "\n",
        "for i, func_str in enumerate(func_test):\n",
        "    func = eval(func_str)\n",
        "    if func(test_x) is None:\n",
        "        results.append(func_str)\n",
        "        continue\n",
        "    sample.append(func(test_x))\n",
        "    if not np.allclose(func(test_x), sample[i]):\n",
        "        results.append(func_str)\n",
        "\n",
        "if len(results) == 0:\n",
        "    print(\"Test PASS!\")\n",
        "else:\n",
        "    print(\"Test FAILED: \" + \", \".join(results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test PASS!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZTlOP46VK1G"
      },
      "source": [
        "### Class `HiddenLayer` \n",
        "\n",
        "#### Hướng dẫn:\n",
        "\n",
        "1. Hàm `forward`:\n",
        "- Hàm nhận vào tham số input $X$ (là output của hidden layer trước theo chiều forward, layer $l-1$).\n",
        "- Tính linear transformation của $X$ ($A^{[l-1]}$): $Z^{[l]} = XW$.\n",
        "- Sau đó tính nonlinear transformation: $A^{[l]} = \\sigma(Z^{[l]})$ với $\\sigma$ là hàm activation.\n",
        "    \n",
        "2. Hàm `backward`:\n",
        "- Hàm nhận vào 2 tham số input `X`(là output của hidden layer trước đó theo chiều forward) với `delta_prev` (delta trước đó theo chiều backward).\n",
        "- Tính delta tại layer $l$: \n",
        "    \n",
        "    $$\\delta^{[l]} = \\frac{\\partial J}{\\partial A^{[l]}}\\frac{\\partial A^{[l]}}{\\partial Z^{[l]}} =  \\delta^{[l+1]} * \\sigma'(Z^{[l]})$$ \n",
        "\n",
        "Chú ý: $*$ operation là element-wise multiplication.\n",
        "- Tính W_grad (without regularization): $\\nabla W^{[l]} = (A^{[l-1]})^T\\delta^{[l]} $\n",
        "- With regularization:  $\\nabla W^{[l]} = (A^{[l-1]})^T\\delta^{[l]} + \\frac{\\lambda}{m} W^{[l]}$ với $\\lambda$ là hệ số regularization (hyperparameter mình sẽ chọn)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfSLDffVYQdc"
      },
      "source": [
        "#### \\[TODO 2\\] Hàm `forward`\n",
        "Định nghĩa hàm `forward` trong class `HiddenLayer` (1đ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4uUr2FKh97J"
      },
      "source": [
        "#### \\[TODO 3\\] Hàm `backward` \n",
        "Định nghĩa hàm `backward` trong class `HiddenLayer` (2đ)\n",
        "  + Tính W_grad (without regularization) (1đ)\n",
        "  + Tính W_grad (with L2 regularization) (1đ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eROUmEreVK1H"
      },
      "source": [
        "class HiddenLayer:\n",
        "    \"\"\"\n",
        "    Abstract hidden layer used in Neural Network.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_neurons, activation, reg = 0.0):\n",
        "        \"\"\"\n",
        "        Constructor for abstract hidden layer.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        num_neurons: (integer) specify number of neurons in this layer.\n",
        "        activation: (string) indicating which activation function to be used.  \n",
        "                        the string must be in [\"sigmoid\", \"relu\", \"tanh\", \"softmax\"]\n",
        "        reg: (float) regularization coefficient to help with overfitting.\n",
        "        \"\"\"\n",
        "        assert activation in [\"sigmoid\", \"relu\", \"tanh\", \"softmax\"], \"Activation must be in [sigmoid, relu, tanh, softmax]\"\n",
        "        self.num_neurons = num_neurons\n",
        "        self.W = None \n",
        "        self.activation = activation\n",
        "        self.reg = reg\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Compute nonlinear function of input X.\n",
        "            X -> LINEAR -> ACTIVATION.\n",
        "            \n",
        "        `activation_function` variable below is equal to this piece of code:\n",
        "        \n",
        "            if (self.activation == 'sigmoid'):\n",
        "                activation_function = sigmoid\n",
        "            elif (self.activation == 'relu'):\n",
        "                activation_function = reLU\n",
        "            elif (self.activation == 'tanh'):\n",
        "                activation_function = tanh\n",
        "            elif (self.activation == 'softmax'):\n",
        "                activation_function = softmax\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X: output of the previous layer (input for the current layer).\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        A: output of the current layer.\n",
        "        \"\"\"\n",
        "        if self.W is None:\n",
        "            W_shape = (X.shape[1], self.num_neurons)\n",
        "            self.W = np.random.normal(0, np.sqrt(2/(X.shape[1]+self.num_neurons)), W_shape)\n",
        "        \n",
        "        activation_function = eval(self.activation) # this returns function variable\n",
        "        #### [TODO 2] START CODE HERE ####\n",
        "        self.Z = X @ self.W\n",
        "        A = activation_function(self.Z) # use `activation_function` function above apply to `Z`\n",
        "        #### END CODE HERE ####\n",
        "        return A\n",
        "\n",
        "    def backward(self, X, delta_prev):\n",
        "        \"\"\"\n",
        "        Compute gradient w.r.t X and W at the current layer.\n",
        "            X <- LINEAR <- ACTIVATION.\n",
        "            W <- LINEAR <- ACTIVATION.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X: output of the previous layer (input for the current layer).\n",
        "        delta_prev: delta dot product with W computed from the next layer (in feedforward direction) \n",
        "                                or previous layer (in backpropagation direction)\n",
        "        \"\"\"\n",
        "        activation_grad_function = eval(self.activation + \"_grad\")\n",
        "        z = self.Z\n",
        "\n",
        "        #### [TODO 3] START CODE HERE ####                                \n",
        "        delta = delta_prev * activation_grad_function(z)\n",
        "        W_grad = np.transpose(X) @ delta # without regularization\n",
        "        #### END CODE HERE ####\n",
        "\n",
        "        #### [TODO 3] START CODE HERE ####                            \n",
        "        W_grad += (self.reg/ X.shape[0]) * self.W  # with L2 regularization\n",
        "        #### END CODE HERE ####\n",
        "        \n",
        "        return W_grad, delta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKWbE4ESVK1I"
      },
      "source": [
        "#### Kiểm tra class `HiddenLayer` được cài đặt\n",
        "\n",
        "Sau khi hoàn thành TODO 2 và TODO 3, bạn có thể kiểm tra class `HiddenLayer` bạn đã cài đặt bằng cách chạy cell bên dưới:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUZJGVMuVK1J",
        "outputId": "f538f852-1e5a-4808-a62d-0b24c0c4b261"
      },
      "source": [
        "np.random.seed(2019)\n",
        "case_1 = HiddenLayer(10, \"sigmoid\", reg=0)\n",
        "case_2 = HiddenLayer(14, \"tanh\", reg=0)\n",
        "case_3 = HiddenLayer(17, \"relu\", reg=0)\n",
        "\n",
        "case_mapping = {0: \"(foward)\", 1: \"(backward W_grad)\", 2: \"(backward delta)\"}\n",
        "\n",
        "with open(\"test/hidden.pkl\", \"rb\") as f:\n",
        "    test_hidden = pickle.load(f)\n",
        "\n",
        "test_x = test_hidden[\"test_x\"]\n",
        "sample = test_hidden[\"sample\"]\n",
        "test_delta = test_hidden[\"test_delta\"]\n",
        "results = []\n",
        "for ind, cl_str in enumerate([\"case_1\", \"case_2\", \"case_3\"]):\n",
        "    hidden_layer = eval(cl_str)\n",
        "    case = (hidden_layer.forward(test_x), ) + hidden_layer.backward(test_x, test_delta[ind])\n",
        "    for i in range(3):\n",
        "        if not np.allclose(sample[ind][i], case[i]):\n",
        "            results.append(cl_str + case_mapping[i])\n",
        "\n",
        "print(\"TEST HiddenLayer WITHOUT REGULARIZATION\")\n",
        "if len(results) == 0:\n",
        "    print(\"Test PASS!\")\n",
        "else:\n",
        "    print(\"Test FAILED: \" + \", \".join(results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEST HiddenLayer WITHOUT REGULARIZATION\n",
            "Test PASS!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3490b6mfJwM"
      },
      "source": [
        "Các bạn nên chắc chắn rằng mình đã pass hết các test cases để làm tiếp."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8HSsFNGfMqi"
      },
      "source": [
        "#### Kiểm tra Hidden Layer với Regularization\n",
        "\n",
        "Các bạn có thể bỏ qua phần test case này nếu chưa làm với Regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYsQBucQfQWq",
        "outputId": "fbcc26be-ed36-4a59-9182-c5f5fac04d9f"
      },
      "source": [
        "np.random.seed(2019)\n",
        "case_1 = HiddenLayer(10, \"sigmoid\", reg=0.9)\n",
        "case_2 = HiddenLayer(14, \"tanh\", reg=0.9)\n",
        "case_3 = HiddenLayer(17, \"relu\", reg=0.9)\n",
        "\n",
        "case_mapping = {0: \"(foward)\", 1: \"(backward W_grad)\", 2: \"(backward delta)\"}\n",
        "\n",
        "with open(\"test/hidden_reg.pkl\", \"rb\") as f:\n",
        "    test_hidden = pickle.load(f)\n",
        "\n",
        "test_x = test_hidden[\"test_x\"]\n",
        "sample = test_hidden[\"sample\"]\n",
        "test_delta = test_hidden[\"test_delta\"]\n",
        "results = []\n",
        "for ind, cl_str in enumerate([\"case_1\", \"case_2\", \"case_3\"]):\n",
        "    hidden_layer = eval(cl_str)\n",
        "    case = (hidden_layer.forward(test_x), ) + hidden_layer.backward(test_x, test_delta[ind])\n",
        "    for i in range(3):\n",
        "        if not np.allclose(sample[ind][i], case[i]):\n",
        "            results.append(cl_str + case_mapping[i])\n",
        "\n",
        "print(\"TEST HiddenLayer WITH REGULARIZATION\")\n",
        "if len(results) == 0:\n",
        "    print(\"Test PASS!\")\n",
        "else:\n",
        "    print(\"Test FAILED: \" + \", \".join(results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEST HiddenLayer WITH REGULARIZATION\n",
            "Test PASS!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRqpw1yWVK1K"
      },
      "source": [
        "### Class `NeuralNetwork`\n",
        "\n",
        "#### Hướng dẫn:\n",
        "\n",
        "1. Hàm `forward`:\n",
        "- Sử dụng hàm `forward` class `HiddenLayer`. Output của layer này [$l$] sẽ là input của layer sau [$l+1$]. Thêm output của layer [$l$] vào list `all_X` \n",
        "\n",
        "\n",
        "2. Hàm `compute_loss`:\n",
        "- Cross-entropy, không regularization: $$J = -\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^C y_{ik}\\log(a_{ik}^{[L]})$$\n",
        "- Cross-entropy, L2 regularization: $$J = -\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^C y_{ik}\\log(a_{ik}^{[L]}) + \\frac{\\lambda}{2m}\\sum_{l=1}^L\\|\\mathbf{W}^{[l]} \\|_2^2$$\n",
        "    \n",
        "    \n",
        "3. Hàm `backward`:\n",
        "\n",
        "- Tính `delta_last` $\\delta^{[L]} $ và `grad_last` $\\nabla W^{[L]}$ theo hàm softmax (để ý superscript [L]): \n",
        "    \n",
        "    $$\\delta^{[L]} = \\frac{\\partial J}{\\partial \\mathbf{A}^{[L]}}\\frac{\\partial \\mathbf{A}^{[L]}}{\\partial \\mathbf{Z}^{[L]}} = \\frac{1}{m} (\\mathbf{A}^{[L]} - \\mathbf{Y})$$\n",
        "\n",
        "    - **Without regularization:**\n",
        "        \n",
        "        $$\\nabla W^{[L]} = \\frac{\\partial J}{\\partial \\mathbf{A}^{[L]}}\\frac{\\partial \\mathbf{A}^{[L]}}{\\partial \\mathbf{Z}^{[L]}}\\frac{\\partial \\mathbf{Z}^{[L]}}{\\partial \\mathbf{W}^{[L]}} = \\delta^{[L]} \\frac{\\partial \\mathbf{Z}^{[L]}}{\\partial \\mathbf{W}^{[L]}} = (\\mathbf{A}^{[L-1]})^T \\delta^{[L]} $$\n",
        "\n",
        "    - **With L2 regularization:**\n",
        "\n",
        "        $$\\nabla \\mathbf{W}^{[L]} = (\\mathbf{A}^{[L-1]})^T \\delta^{[L]} + \\frac{\\lambda}{m} W^{[l]}$$\n",
        "\n",
        "- Tính `delta_prev` $\\delta^{[l]}$ và `grad_W` $\\nabla \\mathbf{W}^{[l]}$ ở các tầng ở giữa:\n",
        "    \n",
        "    $$\\delta^{[l]} = \\left(\\delta^{[l+1]}(\\mathbf{W}^{[l+1]})^T\\right) * \\sigma'(\\mathbf{Z^{[l]}})$$\n",
        "    \n",
        "    - **Without regularization**\n",
        "        \n",
        "        $$\\nabla \\mathbf{W}^{[l]} = (\\mathbf{A}^{[l-1]})^T \\delta^{[l]}$$\n",
        "\n",
        "    - **With L2 regularization**\n",
        "\n",
        "        $$\\nabla \\mathbf{W}^{[l]} = (\\mathbf{A}^{[l-1]})^T \\delta^{[l]} + \\frac{\\lambda}{m} W^{[l]}$$\n",
        "        \n",
        "Mục đích tính `delta` để tính gradient của `W` ở các tầng trước theo chiều forward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sjTUfDesi3b"
      },
      "source": [
        "#### \\[TODO 4\\] Hàm `forward`\n",
        "Định nghĩa hàm `forward` trong class `NeuralNetwork` (0.5đ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP4gB_yyjPoN"
      },
      "source": [
        "#### \\[TODO 5\\] Hàm `compute_loss`\n",
        "Định nghĩa hàm `compute_loss` trong class `NeuralNetwork` (1.5đ)\n",
        "  + Loss without regularization\n",
        "  + Loss of L2 regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07CXsA6GjTXY"
      },
      "source": [
        "#### \\[TODO 6\\] Hàm `compute_delta_grad_last`\n",
        "Định nghĩa hàm `compute_delta_grad_last` trong class `NeuralNetwork` (1đ)\n",
        "  + Without regularization (0.5đ)\n",
        "  + With L2 regularization (0.5đ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJjOZsXWjqz2"
      },
      "source": [
        "#### [TODO 7] Hàm `backward`\n",
        "Định nghĩa hàm `backward` trong class `NeuralNetwork` (1đ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv9USNyilQys"
      },
      "source": [
        "#### \\[TODO 8\\] Hàm `update_weight_momentum`\n",
        "Định nghĩa hàm `update_weight_momentum` trong class `NeuralNetwork`. (1đ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqaUfUyZVK1L"
      },
      "source": [
        "class NeuralNetwork:\n",
        "    \n",
        "    def __init__(self, learning_rate, num_class=2, reg = 1e-5):\n",
        "        self.layers = []\n",
        "        self.reg = reg\n",
        "        self.num_class = num_class\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    def add_layer(self, num_neurons, activation):\n",
        "        \"\"\"\n",
        "        Function to add a hidden layer to neural network.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        num_neurons: hyperparameter that specify nuber of neurons new hidden layer have/\n",
        "        activation: string, indicating which activation function to be used\n",
        "        \"\"\"\n",
        "        assert activation in [\"sigmoid\", \"relu\", \"tanh\", \"softmax\"], \"Activation must be in [sigmoid, relu, tanh, softmax]\"\n",
        "        self.layers.append(HiddenLayer(num_neurons, activation, self.reg))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Do forward propagation in the neural network\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X: raw input X.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        all_X: list of all X computed at each layer. \n",
        "        \"\"\"\n",
        "        all_X = [X]\n",
        "        #### [TODO 4] START CODE HERE ####\n",
        "        for i in range(len(self.layers)):\n",
        "          all_X.append(self.layers[i].forward(all_X[i]))\n",
        "        #### END CODE HERE ####\n",
        "        \n",
        "        return all_X\n",
        "    \n",
        "    def compute_loss(self, Y, Y_hat):\n",
        "        \"\"\"\n",
        "        Compute the average cross entropy loss using Y (label) and Y_hat (predicted class)\n",
        "                    and plus with regularization loss.\n",
        "        Parameters\n",
        "        ----------\n",
        "        Y:  the label, the actual class of the samples. This is one-hot encoding vector.\n",
        "                E.g: [0, 1, 2, 1] => [[1, 0, 0],\n",
        "                                      [0, 1, 0],\n",
        "                                      [0, 0, 1],\n",
        "                                      [0, 1, 0]]\n",
        "        Y_hat: the propabilities of classes (output of softmax).\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        \n",
        "        \"\"\"\n",
        "        #estimating cross entropy loss from y_hat and y\n",
        "        #### [TODO 5] START CODE HERE ####\n",
        "        correct_log_probs = np.log(Y_hat)\n",
        "        data_loss = -1/ Y.shape[0] * np.sum(Y * correct_log_probs) # loss without regularization\n",
        "        #### END CODE HERE ####\n",
        "\n",
        "        #estimating regularization loss from all layers\n",
        "        \n",
        "        #### [TODO 5] START CODE HERE ####\n",
        "        # compute reg_loss\n",
        "        s = 0\n",
        "        for i in range(len(self.layers)):\n",
        "          s = s + np.sum(np.square(self.layers[i].W))\n",
        "        reg_loss = self.reg/ (2 * Y.shape[0]) * s\n",
        "        #### END CODE HERE ####\n",
        "        data_loss += reg_loss # loss with L2 regularization\n",
        "\n",
        "        return data_loss\n",
        "\n",
        "    def compute_delta_grad_last(self, Y, all_X):\n",
        "        \"\"\"\n",
        "        Special formula to compute delta last and gradient last.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        Y:  the label, the actual class of the samples. This is one-hot encoding vector.\n",
        "                E.g: [0, 1, 2, 1] => [[1, 0, 0],\n",
        "                                      [0, 1, 0],\n",
        "                                      [0, 0, 1],\n",
        "                                      [0, 1, 0]]\n",
        "                                      \n",
        "        all_X: raw input data and activation output from every layer\n",
        "        \"\"\"\n",
        "        m = Y.shape[0]\n",
        "        #### [TODO 6] START CODE HERE ####\n",
        "        delta_last = 1/m * (all_X[-1] - Y)\n",
        "        grad_last = np.transpose(all_X[-2]) @ delta_last + (self.reg/ m * self.layers[-1].W)\n",
        "        #### END CODE HERE ####\n",
        "        return delta_last, grad_last\n",
        "\n",
        "    def backward(self, Y, all_X):\n",
        "        \"\"\"\n",
        "        Backpropagation algorithm to compute gradient at each layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Y:  the label, the actual class of the samples. This is one-hot encoding vector.\n",
        "                E.g: [0, 1, 2, 1] => [[1, 0, 0],\n",
        "                                      [0, 1, 0],\n",
        "                                      [0, 0, 1],\n",
        "                                      [0, 1, 0]]\n",
        "        all_X: raw input data and activation output from every layer\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        grad_list: list of gradients we've just computed at each layer.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Compute delta_last and  grad_last from the output\n",
        "        delta_prev, grad_last = self.compute_delta_grad_last(Y, all_X)\n",
        "\n",
        "        grad_list = [grad_last]\n",
        "\n",
        "        for i in range(len(self.layers) - 1)[::-1]:\n",
        "            prev_layer = self.layers[i+1] # previous layer as backward direction\n",
        "            layer = self.layers[i]\n",
        "            X = all_X[i]\n",
        "            #### [TODO 7] START CODE HERE ####\n",
        "            delta_prev = delta_prev @ (prev_layer.W.T) # dot product of delta_prev and W_prev\n",
        "            grad_W, delta_prev = layer.backward(X, delta_prev)\n",
        "            #### END CODE HERE ####\n",
        "            grad_list.append(grad_W)\n",
        "\n",
        "        grad_list = grad_list[::-1]\n",
        "        return grad_list\n",
        "\n",
        "    def update_weight(self, grad_list):\n",
        "        \"\"\"\n",
        "        Update W by gradient descent using the computed gradient.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        grad_list: (list) list of gradients from all layers that computed from backward function above\n",
        "        learning_rate: (float) learning rate for gradient descent.\n",
        "        \"\"\"\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            grad = grad_list[i]\n",
        "            layer.W = layer.W - self.learning_rate * grad\n",
        "    \n",
        "    def update_weight_momentum(self, grad_list, momentum_rate):\n",
        "        \"\"\"\n",
        "        Update W using gradient descent with momentum\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        grad_list: (list) list of gradients from all layers that computed from backward function above\n",
        "        learning_rate: (float) learning rate.\n",
        "        momentum_rate: (float) momentum rate.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"momentum\"):\n",
        "            self.momentum = [np.zeros_like(grad) for grad in grad_list]\n",
        "            \n",
        "        #### [TODO 8] START CODE HERE ####\n",
        "        for i, layer in enumerate(self.layers):\n",
        "          grad = grad_list[i]\n",
        "          self.momentum[i] = momentum_rate * self.momentum[i] + self.learning_rate * grad\n",
        "          layer.W = layer.W - self.momentum[i]\n",
        "        #### END CODE HERE ####\n",
        "            \n",
        "    def predict(self, X_test):\n",
        "        Y_hat = self.forward(X_test)[-1]\n",
        "        return np.argmax(Y_hat, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz3F7s_sowbh"
      },
      "source": [
        "### Training\n",
        "\n",
        "Sau khi định nghĩa các classes HiddenLayer và NeuralNetwork, chúng ta thực hiện huấn luyện (training) mô hình. Trong bài tập này, 2 kỹ thuật training được giới thiệu:\n",
        "  \n",
        "  + Batch training\n",
        "  + Mini-batch training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FfLh29bo1Cd"
      },
      "source": [
        "#### Batch train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0JxaImbVK1N"
      },
      "source": [
        "Định nghĩa hàm `batch_train` để  train trên toàn dữ liệu. Có nghĩa là weights `W` trên mạng neural network sẽ được update trên toàn điểm dữ liệu, thay vì ở mỗi batch điểm dữ liệu như là `mini_batch_train`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFB3Ty-iVK1N"
      },
      "source": [
        " def batch_train(X_train, Y_train, epochs, neural_network, bat=False):\n",
        "    \"\"\"\n",
        "    Using batch train.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train: training data X.\n",
        "    Y_train: training data Y.\n",
        "    epochs: number of iterations that we should use to train.\n",
        "    neural_network: NeuralNetwork object instance above.\n",
        "    \"\"\"\n",
        "    all_loss = []\n",
        "    display_step = 100 if bat else 10\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        all_X = neural_network.forward(X_train)\n",
        "        loss = neural_network.compute_loss(Y_train, all_X[-1])\n",
        "        grad_list = neural_network.backward(Y_train, all_X)\n",
        "        neural_network.update_weight_momentum(grad_list, 0.9)\n",
        "        \n",
        "        all_loss.append(loss)\n",
        "        \n",
        "        if (e+1) % display_step == 0:\n",
        "            display.clear_output(wait=True)\n",
        "            if bat:\n",
        "                y_hat = neural_network.forward(X_train)[-1]\n",
        "                visualize_point(X_train, np.argmax(Y_train,axis=1), y_hat)\n",
        "            plot_loss(all_loss, title=\"Loss epoch %s: %.4f\" % (e+1, loss), color=2)\n",
        "            plt.show()\n",
        "            plt.pause(0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3sb-LfCVK1P"
      },
      "source": [
        "#### \\[TODO 9\\] Mini-batch train\n",
        "Định nghĩa hàm `mini_batch_train` (2đ)\n",
        "\n",
        "**Pseudocode:**\n",
        "\n",
        "```\n",
        "-> For each epoch do:\n",
        "    -> Shuffle data\n",
        "    -> Set initial loss at that epoch equal to 0\n",
        "    -> Calculate number of batches based on batch size and total number of data points\n",
        "    -> For each batch do:\n",
        "        -> all_X := nn.forward() at that batch, Y_hat is equal all_X[-1]\n",
        "        -> compute loss at that batch\n",
        "        -> initial loss += computed loss at that batch\n",
        "        -> grad_list := nn.backward()\n",
        "        -> update weights.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_o23rbaVK1P"
      },
      "source": [
        " def minibatch_train(X_train, Y_train, epochs, batch_size, num_class, neural_network, bat = False):\n",
        "    \"\"\"\n",
        "    Using batch train.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train: training data X.\n",
        "    Y_train: training data Y.\n",
        "    epochs: number of iterations that we should use to train.\n",
        "    batch_size: number of batch at each update.\n",
        "    neural_network: NeuralNetwork object instance above.\n",
        "    \n",
        "    \"\"\"\n",
        "    all_loss = []\n",
        "    display_step = 100 if bat else 10\n",
        "\n",
        "    #### [TODO 9] START CODE HERE ####\n",
        "    for i in range(epochs):\n",
        "      Data = np.concatenate((X_train, Y_train), axis = 1)\n",
        "      np.random.shuffle(Data)\n",
        "      new_X_train = Data[:, : X_train.shape[1]]\n",
        "      new_Y_train = Data[:, X_train.shape[1]:]\n",
        "      \n",
        "      initial_loss = 0\n",
        "      num_patch = len(X_train)// batch_size\n",
        "      remain = len(X_train) % batch_size\n",
        "      for i in range(num_patch):\n",
        "        mini_X = new_X_train[(i * batch_size) : ((i+1) * batch_size), :]\n",
        "        mini_Y = new_Y_train[(i * batch_size) : ((i+1) * batch_size), :]\n",
        "        all_X = neural_network.forward(mini_X)\n",
        "        loss = neural_network.compute_loss(mini_Y, all_X[-1])\n",
        "        initial_loss = initial_loss + loss\n",
        "        grad_list = neural_network.backward(mini_Y, all_X)\n",
        "        neural_network.update_weight_momentum(grad_list, 0.9)\n",
        "\n",
        "      if remain != 0:\n",
        "        mini_X = new_X_train[-remain :, : ]\n",
        "        mini_Y = new_Y_train[-remain :, : ]\n",
        "        all_X = neural_network.forward(mini_X)\n",
        "        loss = neural_network.compute_loss(mini_Y, all_X[-1])\n",
        "        initial_loss = initial_loss + loss\n",
        "        grad_list = neural_network.backward(mini_Y, all_X)\n",
        "        neural_network.update_weight_momentum(grad_list, 0.9)\n",
        "      \n",
        "      all_loss.append(initial_loss)\n",
        "        \n",
        "      if (i+1) % display_step == 0:\n",
        "          display.clear_output(wait=True)\n",
        "          if bat:\n",
        "              y_hat = neural_network.forward(X_train)[-1]\n",
        "              visualize_point(X_train, np.argmax(Y_train,axis=1), y_hat)\n",
        "          plot_loss(all_loss, title=\"Loss epoch %s: %.4f\" % (e+1, loss), color=2)\n",
        "          plt.show()\n",
        "          plt.pause(0.01)\n",
        "\n",
        "    #### END CODE HERE ####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0DirH620RYD"
      },
      "source": [
        "### Bat Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpnU_FBy1IId"
      },
      "source": [
        "#### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X22sH6ug1LqZ"
      },
      "source": [
        "# Thay đổi giá trị của các hyperparameter bên dưới và\n",
        "# quan sát sự thay đổi của loss và quá trình training \n",
        "EPOCHS = 4000\n",
        "LEARNING_RATE = 0.05\n",
        "REG= 1e-5\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUGCZeKq2M-I"
      },
      "source": [
        "#### Định nghĩa hàm `bat_classification`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R_OW98uVK1R"
      },
      "source": [
        "def bat_classification(use_batch_train=True):\n",
        "    # Load data from file\n",
        "    # Make sure that bat.dat is in data/\n",
        "    train_X, train_Y, test_X, test_Y = get_bat_data()\n",
        "    train_X, _, test_X = normalize(train_X, train_X, test_X)    \n",
        "\n",
        "    test_Y  = test_Y.flatten()\n",
        "    train_Y = train_Y.flatten()\n",
        "    num_class = (np.unique(train_Y)).shape[0]\n",
        "\n",
        "    # Pad 1 as the third feature of train_x and test_x\n",
        "    train_X = add_one(train_X) \n",
        "    test_X = add_one(test_X)\n",
        "    \n",
        "    train_Y = create_one_hot(train_Y, num_class)\n",
        "\n",
        "    # Create NN classifier\n",
        "    # Bạn có thể biến đổi thêm/bớt hidden layer, thay đổi hàm activation cho mỗi layer\n",
        "    # và quan sát sự khác biệt trong quá trình train.\n",
        "    net = NeuralNetwork(learning_rate=LEARNING_RATE, num_class=num_class, reg=REG)\n",
        "    net.add_layer(100, 'relu')\n",
        "    net.add_layer(100, 'relu')\n",
        "    net.add_layer(100, 'relu')\n",
        "    net.add_layer(num_class, 'softmax')\n",
        "\n",
        "    if use_batch_train:\n",
        "        #Batch training - train all dataset\n",
        "        batch_train(train_X, train_Y, EPOCHS, net, True)\n",
        "    else:\n",
        "        #Minibatch training - training dataset using Minibatch approach\n",
        "        minibatch_train(train_X, train_Y, EPOCHS, BATCH_SIZE, num_class, net)\n",
        "    metrics = confusion_matrix(test_Y, net.predict(test_X))\n",
        "    print(\"Confusion metrix: \")\n",
        "    print(metrics)\n",
        "    \n",
        "    print(\"Accuracy: \")\n",
        "    print(metrics.trace()/test_Y.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muLdYJbV2dlM"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "4xn572XVVK1V",
        "scrolled": true,
        "outputId": "346a7974-fd73-4985-9770-7ea3ae6d69c8"
      },
      "source": [
        "bat_classification(use_batch_train=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2debzcVPn/3w9tKUsRqL1CKUspiwpKoVxwKWq/ikKRTUV/bIIK9KuCooKCImURVHD5FhTEilgWadmhIiogq7L1FrB0AWxLgUKxl0JbutD1+f1xEieTSTKZmczMvcnzfr3mleSck3OeZJJPnjznJBFVxTAMw+j9bNBuAwzDMIxsMEE3DMPICSbohmEYOcEE3TAMIyeYoBuGYeQEE3TDMIycYIJu5BYRmSci+7fbDsNoFSboBcNErjoicpWIqIjsHEgbKCK3ichyEXlRRI4OrXO0l75cRG4XkYFp101hz7dF5DURWerZ1j+m3IYicrP3H6uIjArli4hcJCKLvN9FIiKBfPVsXOb9roxpY5aIzK9lG4zWYIJuGAFEZD9gp4isy4DVwFbAMcBvRGR3b53dgd8CX/TyVwCXp1k3hT0HAGcCnwB2AIYB5yWs8g/gWOC1iLwxwOHAcGAP4BDgf0NlhqvqAO93YkQd3wW609hutAFVtV+BfsA8YP+I9P7AOOBV7zcO6O/lDQLuBBYDbwAPAxt4eWcArwBvAc8Bn4hptz/wc+Al4D/AFcDGXt4oYD7wA+B1z8ZjAutuDlyDE5IXgR/67Xv5JwGzPBtmAiMC23o6MA1YAtwAbJSwb/oCT+HEToGdvfRNcYK8a6DstcBPvfkfA9cH8nbyym9Wbd0U/9f1wI8Dy58AXkux3nxgVCjtEWBMYPkE4LHA8n+3OabOHb39PBqY3+5j2X6VP/PQDZ+zgA8Ce+I8uH1xwglwGk4gOnBe5g8AFZF3A6cA+6jqZsABOBGN4qfArl79OwNDgLGB/K1xF44hwPHAeK9+gF/hRH0Y8DHgOODLACLyeeBcL+0dwKHAokC9XwAOxInRHsCXEvbBt4GHVHVaKH1XYK2qPh9I+xfge9m7e8sAqOocPBGvtq6IbC8ii0Vk+xibyur25rcSkXcmbEccUXWF7xQe8sI7t4rI0FDer3D//co62jZagAm64XMMcL6qLlTVbtxt/Re9vDXAYGAHVV2jqg+rc9nW4Tzv3USkn6rO88SsDC9OOwb4tqq+oapv4bzaI0NFz1bVVar6IPBn4Asi0scr931VfUtV5wG/CNh2InCxqk5Rx2xVfTFQ56Wq+qqqvgH8CXdBqUBEtsOFH8ZGZA8AlobSluA8cD9/SUx+4rqq+pKqbqGqL0XZFVG3P79ZRNlqRNU1IBBH/xgwFHgP7i7tThHpCyAinwH6qOptdbRrtAgTdMNnG1w4w+dFLw3gZ8Bs4G4RmSsiZwKo6mzgWzgPeaGITBKRbaikA9gEmOp5o4uBv3rpPm+q6vKI9gcB/SJsG+LNbwdUXEQCBGPJK3CiFsU43AUtLMwAy3Def5B34EI81fKrrVuN8Pr+fNr1q9W1zLs4o6oPqepqVV0MnIq7q3mviGwKXAx8s442jRZigm74vIrrdPPZ3kvD84xPU9VhuJDGd0TkE17e9aq6n7euAhdF1P067jZ9d88b3UJVN1fVoLhu6QlHuP3XcXcIYdte8eZfJroTs1Y+AfzMCzf4F4FHvREpzwN9RWSXQPnhwAxvfoa3DICIDMPduTyfYt1qlNXtzf9HVRfFlK+1riQ7FBBgF5zn/rC3b24FBnv7amgddhhNwgS9mPQTkY0Cv77AROCHItIhIoNwoYfrAETkYBHZ2bs1X4ILtawXkXeLyMe9YXRv40R7fbgxVV0P/A74PxF5l1fnEG8ER5DzvGFxHwEOBm5S1XXAjcCFIrKZiOwAfMe3DbgSOF1E9vaG5e3slamVXXECtyelsMwhwG3encOtwPkisqmIjAQOw3VuAvwROEREPuJdlM4HbvUuhNXWrcY1wAkispuIbIHr15gQV1hE+ovIRt7iht7/64dUrsFdjId4d1Kn+XWJyO4isqeI9BGRAbiw1iu4TtDpuDshf9+ciOvY3hN3QTV6Cu3ulbVfa3+4TksN/S4ANgIuBRZ4v0vxRoTgOgvnActxnaNne+l7AE/gbv/fwI2E2Sam3Y1wcfO5uJjyLOCbXt4or96zcB75S8AXA+tuiRPwbpyAjKV8lMtXcSNsluHEZ6/Atu4fKHcucF3K/VQ24gMYCNzu7YOXgKND5Y/20pcDdwAD06yLu9tYBmyfYMt3cAK6FPgD3ugjL28G5SOCov7foV6e4EInb3i/iwHx8j7u7cPlwELP3l1i7BmFjXLpkT//zzSMtuE9AHOdqm7bblsMozdjIRfDMIycYIJuGIaREyzkYhiGkRPMQzcMw8gJfdvV8KBBg3To0KHtat4wDKNXMnXq1NdVtSMqr22CPnToULq6utrVvGEYRq9ERF6My7OQi2EYRk4wQTcMw8gJJuiGYRg5wQTdMAwjJ5igG4Zh5AQTdMMwjJxggm4YhpETCiPoixfDpEnttsIwDKN5FEbQjz8ejjoKnn223ZYYhmE0h8II+kveJ3hX2vfKDcPIKVUFXUSuEpGFIjI9ocwoEXlaRGaIyIPZmmgYhmGkIY2HPgE4MC7T+87h5cChqro78PlsTGsO9rZgwzDySlVBV9WHcN8fjONo3AdxX/LKL8zItkz572dyDcMwckoWMfRdgS1F5AERmSoix8UVFJExItIlIl3d3d0ZNJ0e88wNw8g7WQh6X2Bv4NPAAcDZIrJrVEFVHa+qnara2dER+TpfwzAMo06yeB/6fGCRqi4HlovIQ8Bw4PkM6s4MC7kYhpF3svDQ7wD2E5G+IrIJ8AFgVgb1GoZhGDVQ1UMXkYnAKGCQiMwHzgH6AajqFao6S0T+CkwD1gNXqmrsEEfDMAyjOVQVdFU9KkWZnwE/y8QiwzAMoy4K86SoYRhG3jFBNwzDyAmFE3Qbj24YRl4pnKAbhmHklcIJuo1HNwwjrxRO0C3kYhhGXimMoJtnbhhG3imMoBuGYeSdwgi6hVoMw8g7hRF0wzCMvFMYQbcYumEYeacwgm4YhpF3CifoFks3DCOvFE7QDcMw8krhBN1i6YZh5JXCCbqFXAzDyCuFEXTzzA3DyDtVBV1ErhKRhSKS+Fk5EdlHRNaKyBHZmWcYhmGkJY2HPgE4MKmAiPQBLgLuzsCmpmChFsMw8k5VQVfVh4A3qhT7BnALsDALowzDMIzaaTiGLiJDgM8Av0lRdoyIdIlIV3d3d6NN14TF0A3DyDtZdIqOA85Q1fXVCqrqeFXtVNXOjo6ODJo2DMMwfPpmUEcnMEmcCzwIOEhE1qrq7RnUnTkWSzcMI680LOiquqM/LyITgDt7qpgbhmHkmaqCLiITgVHAIBGZD5wD9ANQ1Suaal0TsFi6YRh5paqgq+pRaStT1S81ZE0LsJCLYRh5xZ4UNQzDyAmFEXTDMIy8UxhBt1CLYRh5pzCCbhiGkXcKI+gWQzcMI+8URtANwzDyTuEEPe+x9LFjYdq0dlthGEY7KJyg55mVK+FHP4IPf7jdlhiG0Q4KJ+hFiKWvW9duCwzDaAeFE/Q8h1zyvG2GYVSnMIJeBM/cMIxiUxhBNwzDyDuFEfQihSPsbsQwiklhBN0wDCPvFEbQzWs1DCPvFEbQi0CRwkqGYVRSVdBF5CoRWSgi02PyjxGRaSLyjIg8IiLDszfTqAW7GzGMYpLGQ58AHJiQ/wLwMVV9P/AjYHwGdhmGYRg1kuYTdA+JyNCE/EcCi48B2zZulmEYhlErWcfQTwD+EpcpImNEpEtEurq7uzNuOh15jjPnedsMw6hOZoIuIv+DE/Qz4sqo6nhV7VTVzo6OjqyaToXFlQ3DyDtVQy5pEJE9gCuB0aq6KIs6s8a8V8Mw8k7DHrqIbA/cCnxRVZ9v3KTmUgRPvQjbaBhGJVU9dBGZCIwCBonIfOAcoB+Aql4BjAXeCVwuTknWqmpnswxulDx76nneNsMwqpNmlMtRVfJPBE7MzKImYV6rYRh5x54UNQzDyAkm6DnCQi6GUWxM0A3DMHKCCXqOMA/dMIpNrxP0F16AK6+EJUvqW78IomcdwIZRTHqdoE+dCiedBC+9VNt6RRC5IlysDMOIp9cJ+uabu2mtHrqJnWEYeafXCvrixfWtn2dP3S5ahlFsep2gv+MdbvrWW/Wtb6JnGEZe6XWCPmCAmy5bVtt6efbMfexiZRjFpjCCXiSKcPEyDKOSXifom27qpibolZiHbhjFptcJer9+0L+/CbphGEaYXifo4MIuJuiVmIduGMWmVwr6ZpvVL+hFED2LoRtGMemVgl6Ph14EkSvCxcowjHgKI+gmdoZh5J2qgi4iV4nIQhGZHpMvInKpiMwWkWkiMiJ7M8uxGLphGEYlaTz0CcCBCfmjgV283xjgN42blcyAAfDII7BuXbNb6l3YXYhhFJuqgq6qDwFvJBQ5DLhGHY8BW4jI4KwMjGLlSje94IL06xQhhm4YRrHJIoY+BHg5sDzfS6tARMaISJeIdHV3d9fd4E9+4qbnngsPP1x3NbnDPHTDKDYt7RRV1fGq2qmqnR0dHXXXs9de8MwzsNNOcMAB8LvfmZgFsbsRwygmWQj6K8B2geVtvbSm8r73uTj6hz8MY8bA0UfD8uXNbrVnYxc1wyg2WQj6ZOA4b7TLB4Elqrogg3qr8q53wd13w49/DDfcAJ/6FKxY0YqWDcMweh59qxUQkYnAKGCQiMwHzgH6AajqFcBdwEHAbGAF8OVmGRvFBhvA978Pu+wCX/gCnHACTJwYXz7PXmyet80wjOpUFXRVPapKvgInZ2ZRnRxxBJx/Ppx9tpv/3OfK86dMcdNf/9qFaQzDMPJGr3xSNI4zz4Thw+GMM2Dt2vI8f8z6o4+23q5WYR66YRSbXAl6375w3nkwZw7cfHO7rWkfNsrFMIpJrgQd4JBDYIcdYMKEdlvSesxDN4xikztB32ADOOYYuOceeP31dltjGIbROnIn6AAHHwzr18MDD7TbktZiHrphFJtcCnpnp3uB19//XplXBNGzGLphFJNcCnq/fvDBD8ITT7TbktZShIuVYRjx5FLQwb3vZfp0WLOm3ZYYhmG0hlwL+urVMHNmdP7xx7tfnjAP3TCKTW4F/f3vd9OwoPuid8017mcYhpEXcivoO+3kpnPmlKdbh6FhGHklt4K+8cawzTaVgp5nLORiGMUmt4IOzksvkqD71HoX8vTTrgPZMIzeTdW3LfZmhg2De+8tT8tzyKVeD32vvRpb3zCMnkGuPfQhQ+C119xTo4ZhGHkn14I+eLB7be6iRe22pDWYh20YxSaVoIvIgSLynIjMFpEzI/K3F5H7ReQpEZkmIgdlb2rtbL21my4IfBBvyZL22GIYhtFsqgq6iPQBLgNGA7sBR4nIbqFiPwRuVNW9gCOBy7M2tB58QX/ttVLam2+2x5ZWYB66YRSbNB76vsBsVZ2rqquBScBhoTIKvMOb3xx4NTsT62fwYDdd0JJPVvcc8tzxaxhGPGkEfQjwcmB5vpcW5FzgWO8j0ncB34iqSETGiEiXiHR1d3fXYW5tbLWVmxZF0M1DN4xik1Wn6FHABFXdFjgIuFZEKupW1fGq2qmqnR0dHRk1Hc+AAe4BI/vQhWEYRSCNoL8CbBdY3tZLC3ICcCOAqj4KbAQMysLARhk4EN54o91WtAbz0A2j2KQR9CnALiKyo4hsiOv0nBwq8xLwCQAReS9O0JsfU0lBrYJ+551w++3Ns6cVWAzdMIpJ1SdFVXWtiJwC/A3oA1ylqjNE5HygS1UnA6cBvxORb+M6SL+k2jP8xYEDaxvZcsghbtozrK+N3mizYRjZkerRf1W9C9fZGUwbG5ifCYzM1rRsGDgQ/v3v8rSg8K1bB336tNYmwzCMZpDrJ0UhOuQSFPQf/rC19jQTf7usE9gwiknhBf2xx1prj2EYRrMohKC//XZ5WvBlXQ880FJzmorF0A2j2BRC0MOY8BmGkUcKKej2Ol3DMPJIIQU9rx56XrfLMIx0FFLQ4zz0X/6yubYYhmE0k0IKepwne9ppzbWl2ZiHbhjFppCCnqcY+h13wEsvtdsKwzB6Arn+SDTApptCv36wZk0pLU+Cfvjh8M53uoeJzEM3jGKTew9dpNJLz5vwFeWbqYZhJJN7QYdKQc+Thx4kbxcqwzBqoxCC/s53li+b8LWeuXPhnnvabYVh5Jvcx9DBPPSewE47uWlPttEwejuF8NDzHkM3DMOAggp6Gg+9N3Y02oXKMIpNKkEXkQNF5DkRmS0iZ8aU+YKIzBSRGSJyfbZmNkZY0G+6qfo6S5c2xxbDMIxmUTWGLiJ9gMuATwLzgSkiMtn7SpFfZhfg+8BIVX1TRN7VLIPrISzop57aHjuajXnohlFs0njo+wKzVXWuqq4GJgGHhcqcBFymqm8CqOrCbM1sjKinRQ3DMPJGGkEfArwcWJ7vpQXZFdhVRP4pIo+JyIFZGZgF4WGLecU8dMMoNll1ivYFdgFGAUcBvxORLcKFRGSMiHSJSFd3d3dGTVdnq61a1lQkK1b0jk/dLV8Ob73VbisMw6iXNIL+CrBdYHlbLy3IfGCyqq5R1ReA53ECX4aqjlfVTlXt7OjoqNfmmhkSvp9oMV/5CnzoQ7BgQbb1Zu2RDxwI73hHtnUahtE60gj6FGAXEdlRRDYEjgQmh8rcjvPOEZFBuBDM3AztbIgtt6x9naeeyq79ri43Xb48uzqjaFTgV6/Oxg7DMNpDVUFX1bXAKcDfgFnAjao6Q0TOF5FDvWJ/AxaJyEzgfuC7qtpjRnKL1L7Ogw+WLz/zDPzxj/W1X6vQ/utfcOWV2ddrGEa+SRVDV9W7VHVXVd1JVS/00saq6mRvXlX1O6q6m6q+X1UnNdPorHn++epl9tgDjj22sXaiLizTprn02bNLaXvuCSedVHv9zRD4U0+FK67Ivt7expNPQgu7fQyjLgrxpCjA5Mlw3XXReYceWplWj1dfD1df7aZ33FH7uq3w0C+9FL72tea309PZe28YMaLdVhhGMoUR9EMOgWOOic6LEsYsxbJZwhuu10IwzWX+/HZbYBjJFEbQk1i8uDKtGYLeKq/fqI233263BYaRDSbouHHiYeLCM42QtaA320Mvgsc/dy5svDFcdVW7LTGMxjFBB5Ytq0x7883s6s9CGF98EU4+Gdata7wuo8SsWW56yy3ttcMwsqBwgn7++Y2tv3Bh7ePJ04RcqsXxv/hFuPxyeOSR+HXMQ68dC4cZeaJwgv6e9zS2/lZbwQc/WNs69QpjcD3fM9+ghf+YCXr2ba1Z0/x2jOJSOEEfPbrxOqZPr2+9JIGMEpTghzj8+WA589Abp5XbeOGFsOGGsGRJ69o0ikXhBH3AgNa36YtGrd8yXbu2so6gh55GjNasyfY1BnmjlR76H/7gpq+/3vy2qmGvecgnhRP0evA7zurFF40kAY7KC3aARnnoaer47GfdAzH+A0y10Fs99B/8ADbbrLZ1WhlDb/d+nTgR+veH555rrx1G9hRS0N///trK77Zbcv60afEnaTC91hM5zkO/+GI4++x09d15p5teckltbQfbbAaTJ0NnZ+13LWn4yU+iRy5F0W5xbQe33uqm06a11w4jewop6EOHVqbdd199df3znzB8OIwbV5n3rW+Vh3iSxCvKQwwKetBDP+MMuOCCyvK9KYZ+9NEwdWrz30CZllZ66DaixmgWhRT0zs7KtM9+tr66/Jdq+XHq7u7S0MJLLnEPLQVDLsuXp+8UC4ZcomK9tQhuT/FEw+Gndotb1H75y1/gt79NLpOGSy+FRx+tb13DqIdCCvoPfgB33w377FNKq3fkQViYPvxhGDkyvux228EWFd9yihaNap2iaerwL171jO5pxkWgpwm6T9COgw6Cr361tFzvfjj1VHc8hOkpF1cjfxRS0Pv2hU9+Ep54ovG6wsIUfA1uuMz69aUnUN98E846K/nJz+CJn2bYYhR77+2m229fvWxS+1kRHvFTbxsvvOD2xf33Z2NPo2XS0FMuXkZ+KaSgp+WV8If2AoQFaelSeO21Un5UZ2gw7Vvfgh//2HUOxqEK997r3tfeypBLmlE59RKuu95OUf8DJPWM3omyJ0lss+q4Nc/caDaFF/TPfCY+L2m4ou9Z+yfpbbfB4MGl/CgRCJ7QfmegH1aJEhRVdyfx7nenE5UowajHK2zGyBOfsKDXK3JphnHWYk+tw0EbwTx1o1mkEnQROVBEnhOR2SJyZkK5z4mIikhEt2PPJOmzckkn8rp17hdXJqpDMyiU4Zh4tXe5JNWRhnpEKWqdiy92o1PqJStBT9OnUAutFPRa61u61F4ZYKSjb7UCItIHuAz4JDAfmCIik1V1ZqjcZsCpwOPNMLRZbLxxfN60abDXXtF5a9a4daM6OCFa0KNi4n36xLcf9eh/2oeTVq1yD4/U4w0mie0ZZ1S3o5a6670byNpDT9NWo9Rr6+abw+GHu7tAw0gijX+zLzBbVeeq6mpgEnBYRLkfARcBuflcwK9/DTvvHJ3nj1uP+jgGpBf0tJ5hVB1JYjRzZnxeNVoZQ2/0wpBV+KKVHno93H57uy0wegNpBH0I8HJgeb6X9l9EZASwnar+OakiERkjIl0i0tXdg764G9cxOW9e/HDGww9PrjMo6P58tZdthakWcgl7jlFvZ4zKq0YzBSyNh/7Xv8Ivf5munkZDLs0a5VIthObz8svZvnu/mfzzn42F24zm03AEUkQ2AH4JnFatrKqOV9VOVe3s6OhotOnMOOQQGDYs2zqjBD1KoNOOK48KuVSL8UP2IZdGSeOhjx4Np1U5mvz90cwYeiNhobTrbL99/F1gT2O//aIfysuSc86xTuNGSHM6vAJsF1je1kvz2Qx4H/CAiMwDPghM7k0dowAf/3i29VUT9ChB+utfyztpozpAG/HQlyyB44+v/hBVKwTdp974dFajU9L0SaTdD3PmwFtvufmo7fJtDdf3xhvp6s+anhBKCtPoB2iKThpBnwLsIiI7isiGwJHAf4MUqrpEVQep6lBVHQo8Bhyqql1NsbhJfOpTbnrUUdl0gtUj6KNHw7HHVpYJzkfVkdR+UPB+/nO45hr3SHoScUKWtgMxya566vR55BH3UjK/Hcgu9p3koaetZ+edS45B1ANjc+bUblczRPfVV6PTFyzIvi2jtVQVdFVdC5wC/A2YBdyoqjNE5HwRObTZBraKI46Af/wDrr02m1u+LGLoUS/nShL0ah56oyND0lzo+vRxrzeIoxEPfeTI0kvJag1ZVSuT9IGRWmzs6qq+TrsF3X/XUPAYuece2GYbuOOO7Nurh55499AbSBWBVNW7VHVXVd1JVS/00saqakV3oqqO6m3eObgTeuTI5GGEtRAl6MHXlabx/oJjj9OEXKLaj3qytJqgx9mWVth8D/Cpp6qPzshqlMuiRfFlli+P78xLI+j12BjloW++eXm9USxeDOPHNz6sMwn/dRBbb11KmzLFTR/vIYOOm/lwW54p/JOicTT6UYsf/ag075/c//u/pTT/gE16l0uUoCd1iqYd5SLivLR//jO63UZCLkFGjKh8EjfuIjFjhvsAdxqCdxsbbOC2Y9Cg0nu+w3Ufc4zrzIvqO0japqwFPU19Y8a448R/z1AzhM1/pXPUsdRTOiSjtnvVKvPcq2GCHkOjH5O+8srk/DS388HPhGXpoYO7G9lvv+h1G/XQk4i7SLzvfbDLLunqWL++XIB87zv8oi6/jB9ieDvhCYlGR7mk2Vdp7sr89wGtWpW+7VpJcg56qqAvXw4bbVTqQzGiMUFP4EMfal7dtXrotcbQw4+Kq7Yu5JKm7qjlpUvT1bFuXXl/QNzrE8LbkSTaUdTioYfLpH2LZlyeb2urBD2rJ2+zIrzd/gN8/ndZeyqzZyc7Ds3GBD2BK66Aj32sOXXXG3JJ66H7Hp6PSPMEfcUK+P3v6xO+egQr+A6dDTaIF7+4C97ee8NvflOe1mgMvVrbwXrSdJi2QtCThla2m7Bt/nmS1XMHzWDVKneXGRyp1mp68O5pP3vsAQ88ANddl33d/knVLA/d9xLquZWOO+GDo26CfO97cOKJbqRE2rrjltMQFPQ0wxbD+/rJJ+HrX3fzacQ1ysZVq+A//ym3KWxjLfX5hD3leh9q2nvv+A7pdodc/vjH6t98jbtA9mRB90Okf/tb+2zowbun5zBoUPZ1hj30qE7Y3/0uvjyUn5CLF5cvH3dceZlaPPRwmz5xgu6PaknzfdCsPPTgCZ425JLUUVnrsMUjjigfJRLnUUbV1+yQy4oV7qJ1zDHJ5Zoh6FOmwEsvxedPm+Y82BNOSK4nTtCzGoXWDHpCh60JegqCn6rLivAThVddVVkm6GH55U45pTIN3NePok7+oPClPWl94Q7XF/cK11q8p1o89Li8sIeeNuQS9brjJMFMEuA776ysO229SSGPLAS92v8c1X5Wgr7vvrDDDuVpQW+8Xz83ffLJ5Hri9mdP9tCT7rZbRQ/ePT2HgQPdAf+ud2VX5/Tpbpr2Pdf+Af3vf1emgfOOk27zg1Q7af2XlYXri/PQGxH0eh7AiRP0ah762rWV25D0zps0HnWcF5/0XyT1n2Qh6NXe49PKkMvEibDZZqVnMPz6444ln94Yckn6WE2r6MG7p+cxZ477Ivx3v5tdnWnf4xF1AoQ9rGpD5dLeEq5cWVl/lA1/+lP55/Hq9dCThDsuPdhmLSGX8AU0KeSSphMz6inguHXShICyEPRqYtlKQf+z9/7Vf/3LTdOOpunJnaJLl8Imm7gPzQcxD72XMWAAHHggXHRR6eGMVhHlyQdPyFdfje4kDXqZaU9av61qIZdDDy3/PF4aMYjy0JOEOy496LHFiV/UUMJaBD2Nh+6LZz2dolHCW8sdzKJF0XVU8xSbGXKpRlphjjv2eoKgP/OMc3rOPbc8vdqFtBX0gN3T+xBxIxxOOQX23LM1bUYdLK+/Xpq/997op0ODJ2/akzbuoZZmhFxUk+udNcvt56Ata9eWC3FcyCUsyFEhl3pj6PLXvdoAABZgSURBVEFbouppNOQS5/n7rF7tOuu/9rV4m1odcol6jsDvKwpfdGv10P1t6gmCHne8maD3YjbZBH71K/e+kp12an57UQdLeHhUUidg0nDHMP5n5rIS9CjhCK6b5Ikfdhhcdll530HYQ49rv9GQS5qneeM89EZDLtUE3b/oTpoUb1Pa5w2C840IetS6fn+ML+z1euj+NqUd5TJlClxySbqytRIn6BZyyQnPPuseSd5sM/j+91vXbviJtPABNXdu6aC75prSSRJ8pUASSSGXqAtEnCdbbSRL3IUi7oK0bl1pnb5948Wo0ZBLmnHj9XjoaQS92sUkSTzSxtAbDbmsXg3Dh7u7w3B9YfyRLq3y0PfdF771rXRlayWuz8Y89JzQt697Mf/SpfCDH7Su3Thx8pk6tXTQPflkScjTCnpQNN5+u/yAjXroKUmYfWoNuUSdPHGCHj7ZWxlyqWfYYlIMvZqH7q8bZVs1TzHprq0WQZ83z41e8cM+Ubb6d6/Dh5fb5v9XTz9d+VRzVF31hlyi3sLZKHF9No18JSwrTNAzZsAA98f+6U/Nb+vBB8uXwwfYzJnlJ60v5P4olmqEh0UGBSh4UfDLxQ3BDApM+ORdvz75QuCfHOHXEQdvwePEqJaQy4QJle3XEnLJOoZere2k4a71hFx8ahEj334/DBK1PUOHli8H/6uXX4a99oJvfjO+bp96BT34NG9WWAy9YGywARx8sPNqFy5049dHj86+neD71aHyJLjwwmhBX7EiXf1BMQmKaLAuKKVHeVphu847rzxPNTmG7p88wfaCtgQvWnEx9KCd1e5qotavJ+TSzBi6SPmrmAFefLH0OuSsRrksWOCG6FYbheQLepytwbL+dObMUqe+/6rgIFmNctl449rKpyHuzq3XxNBF5EAReU5EZovImRH53xGRmSIyTUT+LiI7RNVTNPr3h44O90rUu+6Cq69ubntJIgIlUUzziH64vtWrSyeVSLnAVgvlBA/0V14pz0vy0NevLwlC8GIRFPQJE+I7RcMnXviiFLYtqv3g+lGEhSpNvVnE0MN3gMOGlV6HXE3Qo7YrStBPPNF9tjB8J+jjHw/1CHpw/b594230qbVT1K8z7bMXtRAX7uoVHrqI9AEuA0YDuwFHichuoWJPAZ2qugdwM3Bx1ob2ZvyD+rjj3EGwaFH1R5/rwT9Z3v3uyjQojTRI46GvX19+UnV3l9/2Rgl6nIceHnIY5P77kz37aoLuL0NjnaJJdmcVcgnblTSGvFoMPVxX1D5udJRLtePE/0+SBN0nanuC/SBhGo2hx/VtZEGcoPeWGPq+wGxVnauqq4FJwGHBAqp6v6r6f/9jwLbZmpkvBg50sUNV99rZ227Lpt5f/MJNg9+FDAqv/xKtqFvcMLfdVn4y/L//V36SRAl6UADC4RqfsKCecUZ8TD9J0IP1xN2ORz3IU4+gZ90pmhRDD18gqj0slWRTHEkhl6iO0hdeiK7HH2WVFENP8tCbKeg+zRB0v85e6aEDQ4CXA8vzvbQ4TgD+EpUhImNEpEtEurq7u9NbmWO+8hU4/HB3cLz9tvOuf/5z+NSnaq/Lj6kH44ZPPVWanz/fTefNK6X5XnuYs88uP6leeKFcDJ99tjTvC3rwE29hAQ6XDRLnCaYJuUC8oPthiaCA1jLK5YEH3DTpRPXzwqJXb8ilmoceviCFvcHgNtbzYFHUhfjEE6Pr8S/EvqAnjVaJupPxy/c2Qa/mobeTTDtFReRYoBP4WVS+qo5X1U5V7ezo6Miy6VzQv78TytNOcw8N/fSn9dUTPOhnzy7NB58s9Yk7WWfNqrw9DgpbsF5fpINPCl56afm6PlEnfT0eenAMfjC2H+Tkkyvrq8VD94fjpRH0T386fb2NCHp4/4VFZdWqylAIVI5SgtIFK86mKA8+iP8f+IIc9aUef39Heej+hdx/A2OQWjtFly+Hxx6rTE8rso8+6o4f39lRhYcfjt7uOEGPOw5bSRpBfwXYLrC8rZdWhojsD5wFHKqqMVFRoxbOOMMdNHPmuPG6//hHuvX69IFrr01X9sYb4/PC4hEUw+D43ihBv/760nzwnRcvvljZTpygP/RQqc2woG+4YaVd1R5sqjXkEq4fSt/89IkS+zVrGo+hx4Vlqj1DsHJlSViD+2jAAPjc58rbD373NslDD6f7hEMuURfrpAuU/79n4aEPGOA+GRl2WtIKuv9AoH8Rv+oq+OhHKz88HrQl6mIK0ReoVpFG0KcAu4jIjiKyIXAkMDlYQET2An6LE/OU32430jJsmHswY+RINwzywQdh3Dj3NNytt1YOBfQP7kYJv5sjODpm5szSvH8gjx9fSvO/AQnlX3wKhmV84kIuJ55YEo2gWMycGR1yGTcuup4049CTCLYVviBFifKzz5YLycsvl+eniaH7eUF777svvgPZ5+23S/vMFxa/Lv8x/KgLX5SHHhXvDrcFtQl6sM4kQY8bh15tlEvYOUgr6L5n/uijbuqHFOfMcdP16yuHhcYJejs/wlFV0FV1LXAK8DdgFnCjqs4QkfNF5FCv2M+AAcBNIvK0iEyOqc5okI4O5zmceio8/jh85jMwdmy5mG26qXtCb++9a6//ootK8+GROEFhCnoub75ZWU/wqzXhEz0sDsGTMCy4/rrBOk4+ObpTFJI7DVevTo6hB/sFggTrD16oIFroVqwor3fBgvL8r37VTdN0igY98qeeqvTQw7f3QQ/dF8rw6yii9lFUJ2xUx3dw/bAgR4Vc/O2JiqH7F/IoQb/vvuh6qsXQw/9H2o7Kz3/eTb/3PTcN76OLLnLDQu+7r3o4LLw9q1alHyrcKKli6Kp6l6ruqqo7qeqFXtpYVZ3sze+vqlup6p7e79DkGo2sOfVUuOkmuOGG0kHf1RUttlHss4/zsE87zb14DNzrDIL85Cf12RY+8B96qHw5KOi//315nn+S3HJLeXpQYIJPA0aFb/z2v/jFcnGeMqV8+eabK9eF+FcehPN8wp2vYY/N70ANCrr/PhQ/7cc/dtPgdopUXhw32qh8+e23S/vAD7mEv66UFBeO6rQMz/udzWk89KQYuv9+lyhB918QF7bvr3+tLBsk7v1Gq1e7MEr4WHziCddf5NcfDFMF8V/p8eyz8WGvOEHfaCN319yMp1bD2JOiOeKII+ALXyhP22ILdwI/8URlx12Qb34TTjrJnZz+bWaYrIZlhZ+anTu3NB++iPgnaNhjC55UwU/1RYV0gnYH8+++u1yEfvSjaHuD699wQ3lecIioz9Kl5fX6o4uS6vU/AOELoB/SCl9AwmLSv3+5QAc99OnTXV74riLp/S8XXFBKC25DMDbtd3g3GnLxQ3pRgh4meKH2RXnPPSvvUMKC7u+/r37Vfcd00iRnk/9xig98AHbbrfrTzj4dHZV3QD7+usFRZEHCobdmYIJeEPbZx3lqqs4jmTXLDY/0GTasNL/11tHv2c6KsCgFwzzh8ERcfP2ZZ0rzwdvZU08tLxeM9YMLU/msWVMuAHGeV1BUr7uuXBB//Ws33XnnUtro0eUCdPjhlXUuXx4dagpfNIM2LVtWKTivvlq+j4IxdHCjpcJ3aUmdsUGC9QSf8A2HXHxRTQq5+H0PQQ/5kUfK60si2J/j7wP/IhjsoA/bsP/+bvrww2765pvuonXAAeWjfPxt8afhPgU/fDlgQGl/P/OM255993UXiuB/E3zds08rRr+YoBeQ97zH/U47zQno3XdXdqJefjl85zvutbtBUWnGuzHqwX9IKsxNN5Uv/yw0gDY4suPmm6u/gnjlykqhjxq7H/bWqj1l+eab5YLut5sk6OecE33RmTixND93bvk2rViRPFrJbzfK3uB6QS/fv1vy2/nznyvb8d/T7m+Pf9EO7l9fUNN8Vzco6OH/LCjoK1dGP0U8alSpTf9CELxz8uv3/xO/Dd9ZmDq1lB50IKZMcb+jjoq+owlerKIueFljgl5wtt4aPvnJaO/hF79wced+/UrDJ5ctcwfm2LFuKCU40UzjfaQddpmWbbapXibqLYo+M2a4bdl001KaP8rB52tfq+zQCnb4ghPZ8IigavHS7u5yEfUvNEEBiLrg+HHnIMG7mi99qfxCMXu2+2xi0K6ggPp3LMF6fUEMbnd4v0C5bY88Ui5o/pjwuE7uILfcUr3TMCzowf0UfEYxfIfi48fGgyOdgqE+36P2950fmgvv77Cg+xeTcP9G1HuTov67rDFBN1IzbJjrcO3f3w2VHD7cnVinn+5uPRcscAe4quu8mjPHeXb77OOefN13X3egjxvnPj4QDLXceKOL94MblRH1dsqjjipfTjvWPombbir/4tRHPlKef/XVlQL+/veXL++zT+X45/BQ0vCdw4gR5Sf7vHmVfRenn17ZJxAlrGPHli8H7yDOOKNcSLbeulzwR45006Bg+vUFxdh/rYSPank769aVL/tfCwqK3MKF8cLd1VUZeglue9C+iRPLL4bB8MaNN0Z3jF9+uZtuuKE7fqE8/DZjhpvedJOzww9TLV5cbvPKleX70/+IRp8+5dv6mc+4aXCfTJ7szpPwC+oyRVXb8tt7773VMJK4917V225Tff111RNPVF28WLW7W/XjH1cF1bfeKpW97z6X5v/OPLN8+ZZbypeDv8MOU7366vj8Zv0237x6mZNOarydd70rOX/tWtXhw8vTVCvTgr/x41X337+0vOOOqt/9bnmZ9etV+/YtLY8cqfrTn0bXd+yxqitWlKcdf3zp//X/c//30kvx//Wzz5YvP/dcaX7LLVUHD3bz/jT8e/zx0vx736v6j3+U53/5y9HrnX565T58/vnosn/4Q/3nBdClGq2rkYmt+JmgG1mzdq1qV5fqI4+45fXrVe+8U/Xvf3fLjz/uTsYXXlCdMUP1Yx9T3XVX1XvucfkXXODOiF12UZ01S7Wzs3QC7ref6h13lJY/9anKkzR8ou+wQ2WZc89NFtd2/TbZpHz5mGNUN95Y9QMfiF9ngw0q04IXj//5n8r8r389vr77769MW7XK/TfhfXnZZfH1TJhQvrzvvq3ZhzvuWGn71KnRZY85pv7j3ATdMFKybp27EPisX+8uEIsWldJefll12bLS/PjxpYvGypWqDzyg+o1vqD74oEt78knVSy5RvfBC1TVr3O/nP1d93/tUf/lL1SuvVB0xwgnktde6dY47rnTyn3uu8yz95d13V/3978vFYtKkcsEYObJ8efx41bPOcuvWIlIXXFC60KX5HXxw9TJbb12bDePGqYq0RpSDv+uvb2z9jTdWveGGyvQ773THWb2YoBtGTogSguXL3XTJEneH8pe/uHJvv606Z44LWfksWKA6ZYqbX7vWhQS+8Q3V885zIa01a5yQnX22u9D4Ya0jj1S95hrVefPcxWbrrVU/+1nV//xH9ZvfdBemSy5x7T3zjAuXXHSR6u23V96VnHuus2/ZMtWnny7PGzJE9a67KkVwk01c6GTtWtWNNnJpRxzhwiZnneW84WD5YcNUTzutXFw/9zk3/8YbpXDRggWlMl/6Uml+v/3cdu+3n1ueNUt11KjostV+ffqU5oPOQr0kCbq4/NbT2dmpXV1dbWnbMIz2sGZN/MurVq50nYv+iBRfBqdNg8GDYautSmUXLXLfFfARcWVFXOdr376l/OnTYfPNYdAg9/zFiBGuE/vpp904dVXXWdmnj+soFXEd3xtv7Opav951JoMb8vinP8E3vuHWGz8eDjqoNKR04kQ3Vv2JJ9wL6vr1cw/0qbrO1EMOaXwfishUVe2MzDNBNwzD6D0kCboNWzQMw8gJJuiGYRg5wQTdMAwjJ5igG4Zh5AQTdMMwjJxggm4YhpETTNANwzByggm6YRhGTmjbg0Ui0g28WLVgNIOA16uWaj091S7oubaZXbVhdtVGHu3aQVU7ojLaJuiNICJdcU9KtZOeahf0XNvMrtowu2qjaHZZyMUwDCMnmKAbhmHkhN4q6OPbbUAMPdUu6Lm2mV21YXbVRqHs6pUxdMMwDKOS3uqhG4ZhGCFM0A3DMHJCrxN0ETlQRJ4TkdkicmYb2p8nIs+IyNMi0uWlDRSRe0Tk3950Sy9dRORSz9ZpIjIiQzuuEpGFIjI9kFazHSJyvFf+3yJyfJPsOldEXvH22dMiclAg7/ueXc+JyAGB9Ez/ZxHZTkTuF5GZIjJDRE710tu6zxLsaus+E5GNROQJEfmXZ9d5XvqOIvK418YNIrKhl97fW57t5Q+tZm/Gdk0QkRcC+2tPL71lx75XZx8ReUpE7vSWW7u/4r5N1xN/QB9gDjAM2BD4F7Bbi22YBwwKpV0MnOnNnwlc5M0fBPwFEOCDwOMZ2vFRYAQwvV47gIHAXG+6pTe/ZRPsOhc4PaLsbt5/2B/Y0ftv+zTjfwYGAyO8+c2A573227rPEuxq6z7ztnuAN98PeNzbDzcCR3rpVwBf8+a/DlzhzR8J3JBkbxPsmgAcEVG+Zce+V+93gOuBO73llu6v3uah7wvMVtW5qroamAQc1mabwNlwtTd/NXB4IP0adTwGbCEig7NoUFUfAt5o0I4DgHtU9Q1VfRO4BziwCXbFcRgwSVVXqeoLwGzcf5z5/6yqC1T1SW/+LWAWMIQ277MEu+JoyT7ztnuZt9jP+ynwceBmLz28v/z9eDPwCRGRBHuztiuOlh37IrIt8GngSm9ZaPH+6m2CPgR4ObA8n+SDvxkocLeITBWRMV7aVqq6wJt/DfA/Z9tqe2u1o5X2neLd8l7lhzXaZZd3e7sXzrvrMfssZBe0eZ954YOngYU4wZsDLFbVtRFt/Ld9L38J8M5W2KWq/v660Ntf/yci/cN2hdpvxv84DvgesN5bfict3l+9TdB7Avup6ghgNHCyiHw0mKnuvqntY0F7ih0evwF2AvYEFgC/aJchIjIAuAX4lqouDea1c59F2NX2faaq61R1T2BbnJf4nlbbEEXYLhF5H/B9nH374MIoZ7TSJhE5GFioqlNb2W6Y3iborwDbBZa39dJahqq+4k0XArfhDvT/+KEUb7rQK95qe2u1oyX2qep/vJNwPfA7SreQLbVLRPrhRPOPqnqrl9z2fRZlV0/ZZ54ti4H7gQ/hQhZ9I9r4b/te/ubAohbZdaAXulJVXQX8gdbvr5HAoSIyDxfu+jhwCa3eX410ALT6B/TFdV7sSKnjZ/cWtr8psFlg/hFc3O1nlHesXezNf5ryDpknMrZnKOWdjzXZgfNkXsB1Cm3pzQ9sgl2DA/PfxsUIAXanvANoLq5zL/P/2dv2a4BxofS27rMEu9q6z4AOYAtvfmPgYeBg4CbKO/m+7s2fTHkn341J9jbBrsGB/TkO+Gk7jn2v7lGUOkVbur8yE5dW/XC91s/j4nlntbjtYd7O/hcww28fF/v6O/Bv4F7/wPAOoss8W58BOjO0ZSLuVnwNLs52Qj12AF/BdbzMBr7cJLuu9dqdBkymXKzO8ux6DhjdrP8Z2A8XTpkGPO39Dmr3Pkuwq637DNgDeMprfzowNnAOPOFt+01Afy99I295tpc/rJq9Gdt1n7e/pgPXURoJ07JjP1DvKEqC3tL9ZY/+G4Zh5ITeFkM3DMMwYjBBNwzDyAkm6IZhGDnBBN0wDCMnmKAbhmHkBBN0wzCMnGCCbhiGkRP+P2h+sI/cpSLJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion metrix: \n",
            "[[1464   23    0]\n",
            " [  66 1034    8]\n",
            " [   0   59  346]]\n",
            "Accuracy: \n",
            "0.948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHRIZzum4Q7v"
      },
      "source": [
        "### MNIST Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAv5PeHq4YRQ"
      },
      "source": [
        "#### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89OE4gdG4bwy"
      },
      "source": [
        "# Thay đổi giá trị của các hyperparameter bên dưới và\n",
        "# quan sát sự thay đổi của loss và quá trình training\n",
        "EPOCHS = 300\n",
        "LEARNING_RATE = 0.01\n",
        "REG= 1e-5\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFDjFonm5buD"
      },
      "source": [
        "#### Định nghĩa hàm `mnist_classification`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EDiru6zVK1S"
      },
      "source": [
        "def mnist_classification(use_batch_train=True):\n",
        "    # Load data from file\n",
        "    # Make sure that fashion-mnist/*.gz is in data/\n",
        "    train_X, train_Y, val_X, val_Y, test_X, test_Y = get_mnist_data(1)\n",
        "    train_X, val_X, test_X = normalize(train_X, val_X, test_X)    \n",
        "    \n",
        "    num_class = (np.unique(train_Y)).shape[0]\n",
        "\n",
        "    # Pad 1 as the third feature of train_x and test_x\n",
        "    train_X = add_one(train_X)\n",
        "    val_X = add_one(val_X)\n",
        "    test_X = add_one(test_X)\n",
        "    \n",
        "    train_Y = create_one_hot(train_Y, num_class)\n",
        "    val_Y = create_one_hot(val_Y, num_class)\n",
        "\n",
        "    # Create NN classifier\n",
        "    net = NeuralNetwork(learning_rate=LEARNING_RATE, num_class=num_class, reg=REG)\n",
        "    net.add_layer(128, 'relu')\n",
        "    net.add_layer(256, 'relu')\n",
        "    net.add_layer(100, 'relu')\n",
        "    net.add_layer(64, 'relu')\n",
        "    net.add_layer(num_class, 'softmax')\n",
        "    \n",
        "    if use_batch_train:\n",
        "        #Batch training - train all dataset\n",
        "        batch_train(train_X, train_Y, EPOCHS, net)\n",
        "    else:\n",
        "        #Minibatch training - training dataset using Minibatch approach\n",
        "        minibatch_train(train_X, train_Y, EPOCHS, BATCH_SIZE, num_class, net)\n",
        "    metrics = confusion_matrix(test_Y, net.predict(test_X))\n",
        "    print(\"Confusion metrix: \")\n",
        "    print(metrics)\n",
        "\n",
        "    print(\"Accuracy: \")\n",
        "    print(metrics.trace()/test_Y.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9dLdHlaVK1X",
        "scrolled": false,
        "outputId": "b5040732-4d77-4dde-fb22-bae3931dc01e"
      },
      "source": [
        "mnist_classification(use_batch_train=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading fashion MNIST data...\n",
            "Done reading\n",
            "Confusion metrix: \n",
            "[[836   1  25  19   3   3 107   0   6   0]\n",
            " [  5 978   1  13   1   0   1   0   1   0]\n",
            " [ 17   1 843   7  65   2  62   0   3   0]\n",
            " [ 27   9  12 883  35   0  28   0   6   0]\n",
            " [  2   2  82  29 828   0  57   0   0   0]\n",
            " [  0   0   0   1   0 960   0  19   2  18]\n",
            " [113   2  78  26  62   0 707   0  12   0]\n",
            " [  0   0   0   0   0  15   0 962   1  22]\n",
            " [  7   0   3   4   4   4   6   8 964   0]\n",
            " [  0   0   0   0   0   9   1  24   0 966]]\n",
            "Accuracy: \n",
            "0.8927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd7zbo_zVK1Y"
      },
      "source": [
        "## II. Thực hiện Deep Neural Network với Tensorflow\n",
        "\n",
        "Phân loại tập dữ liệu Bat và MNIST với tensorflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m51lTqD6rW73"
      },
      "source": [
        "### Import các thư viện cần thiết"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozo-4la8VK1Z"
      },
      "source": [
        "import tensorflow as tf\n",
        "L = tf.keras.layers\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N8tbrbxVK1b"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljzbayIbrmGw"
      },
      "source": [
        "### Cài đặt class `DNNModel`\n",
        "\n",
        "Trong phần này bạn có các TODO sau:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJNmEejir4pM"
      },
      "source": [
        "#### \\[TODO 10] Cài đặt kiến trúc mô hình\n",
        "\n",
        "Yêu cầu: xây dựng kiến trúc mạng với `tf.keras.Sequential()` để chồng các hidden layers và output layer với nhau. (1đ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmKqmC97tttR"
      },
      "source": [
        "#### [TODO 11] Cài đặt hàm tính accuracy\n",
        "\n",
        "Cài đặt hàm tính accuracy. (1đ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDJZnuTEvgzH"
      },
      "source": [
        "#### \\[TODO 12\\] Cài đặt hàm train\n",
        "Cài đặt các bước để train mô hình trong class `DNNModel`. (2đ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO7GgBTzVK1c"
      },
      "source": [
        "class DNNModel:\n",
        "    def __init__(self, hidden_layers, num_classes, activation, epochs, optimizer):\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.num_classes = num_classes\n",
        "        self.activation = activation\n",
        "        self.epochs = epochs\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "        #Hidden layers and output layers are stacked to form a model\n",
        "        self.model = tf.keras.Sequential()\n",
        "        #### [TODO 10] START CODE HERE ####\n",
        "        #Add hidden layers\n",
        "        for i in range(len(self.hidden_layers)):\n",
        "          self.model.add(tf.keras.layers.Dense(self.hidden_layers[i], activation = self.activation))\n",
        "        \n",
        "        #Add output layer\n",
        "        self.model.add(tf.keras.layers.Dense(self.num_classes, activation = \"softmax\"))\n",
        "        #### END CODE HERE ####\n",
        "\n",
        "    def loss(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Compute loss function.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y_hat: output of the last layer (softmax layer).\n",
        "        y: labels/targets in our data.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        Loss w.r.t y_hat and y. Should be a scalar.\n",
        "        \"\"\"\n",
        "        return tf.reduce_mean(-1 * tf.reduce_sum(y * tf.math.log(y_hat+1e-6), axis=1), axis=0)\n",
        "\n",
        "    def accuracy(self, y_hat, y):\n",
        "        \"\"\"\n",
        "        Compute accuracy score.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        y_hat: output of the last layer (softmax layer).\n",
        "        y: labels/targets in our data.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        Accuracy w.r.t y_hat and y. Should be a scalar.\n",
        "        \n",
        "        \"\"\"\n",
        "        #### [TODO 11] START CODE HERE ####\n",
        "        y_check = tf.argmax(y, axis = 1)\n",
        "        y_hat = tf.argmax(y_hat, axis=1)\n",
        "        count = [1 for i in range(len(y_hat)) if y_check[i] == y_hat[i]]\n",
        "        acc = len(count)/ len(y)\n",
        "        #### END CODE HERE ####\n",
        "        return acc\n",
        "\n",
        "    def train(self, x_train, y_train):\n",
        "        all_loss = []\n",
        "        all_acc = []\n",
        "        for e in range(self.epochs):\n",
        "            #### [TODO 12] START CODE HERE ####\n",
        "            self.model.compile(optimizer = self.optimizer, loss = \"categorical_crossentropy\", metrics = ['accuracy']) \n",
        "            self.model.fit(x = x_train, y = y_train)\n",
        "            y_hat = self.model(x_train)\n",
        "            accuracy = self.accuracy(y_hat, y_train)\n",
        "            loss = self.loss(y_train, y_hat)\n",
        "            #### END CODE HERE ####\n",
        "            \n",
        "            print(f\"\\rEpoch: {e}... Training loss: {loss}... Accuracy: {accuracy}\")\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        Y_hat = self.model(inputs)\n",
        "        return tf.argmax(Y_hat, axis=1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H7YE6M4uccG"
      },
      "source": [
        "### Bat classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOEopv9gVK1f"
      },
      "source": [
        "def tf_bat_classification():\n",
        "     # Load data from file\n",
        "    # Make sure that bat.dat is in data/\n",
        "    train_X, train_Y, test_X, test_Y = get_bat_data()\n",
        "    train_X, _, test_X = normalize(train_X, train_X, test_X)    \n",
        "\n",
        "    test_Y  = test_Y.flatten()\n",
        "    train_Y = train_Y.flatten()\n",
        "    num_class = (np.unique(train_Y)).shape[0]\n",
        "\n",
        "    # Pad 1 as the third feature of train_x and test_x\n",
        "    train_X = add_one(train_X) \n",
        "    test_X = add_one(test_X)\n",
        "    \n",
        "    train_Y = create_one_hot(train_Y, num_class)\n",
        "    \n",
        "    # DNN parameters\n",
        "    hidden_layers = [100, 100, 100]\n",
        "    learning_rate = 0.001\n",
        "    epochs = 200\n",
        "    activation = tf.nn.relu\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    #Initialize model\n",
        "    dnn = DNNModel(hidden_layers=hidden_layers, num_classes=num_class,\n",
        "                   activation=activation, epochs=epochs, optimizer=optimizer)\n",
        "    #Train\n",
        "    dnn.train(train_X, train_Y)\n",
        "\n",
        "    # TEST\n",
        "    # Confusion matrix\n",
        "    metrics = confusion_matrix(test_Y, dnn.predict(test_X))\n",
        "    print('Confusion matrix:')\n",
        "    print(metrics)\n",
        "    \n",
        "    print(\"Accuracy: \")\n",
        "    print(metrics.trace()/test_Y.shape[0])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLs06JB9VK1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a1f40e9-4df4-40a6-8807-819fe4689de3"
      },
      "source": [
        "tf_bat_classification()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading bat data...\n",
            "EOF Reached\n",
            "Done reading\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9411 - accuracy: 0.5196\n",
            "Epoch: 0... Training loss: 0.5773306488990784... Accuracy: 0.74475\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.5223 - accuracy: 0.7636\n",
            "Epoch: 1... Training loss: 0.4932563900947571... Accuracy: 0.7691666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.4303 - accuracy: 0.8091\n",
            "Epoch: 2... Training loss: 0.40476253628730774... Accuracy: 0.8065833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.4011 - accuracy: 0.8141\n",
            "Epoch: 3... Training loss: 0.34448763728141785... Accuracy: 0.8550833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.3391 - accuracy: 0.8509\n",
            "Epoch: 4... Training loss: 0.3896999955177307... Accuracy: 0.8195833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.3539 - accuracy: 0.8399\n",
            "Epoch: 5... Training loss: 0.30487459897994995... Accuracy: 0.8614166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.3111 - accuracy: 0.8589\n",
            "Epoch: 6... Training loss: 0.32975834608078003... Accuracy: 0.84975\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.3211 - accuracy: 0.8554\n",
            "Epoch: 7... Training loss: 0.2760404348373413... Accuracy: 0.8803333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2966 - accuracy: 0.8676\n",
            "Epoch: 8... Training loss: 0.282711386680603... Accuracy: 0.8765833333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2749 - accuracy: 0.8770\n",
            "Epoch: 9... Training loss: 0.2528645694255829... Accuracy: 0.8971666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2664 - accuracy: 0.8794\n",
            "Epoch: 10... Training loss: 0.2564235031604767... Accuracy: 0.887\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2613 - accuracy: 0.8850\n",
            "Epoch: 11... Training loss: 0.25039786100387573... Accuracy: 0.8885\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2595 - accuracy: 0.8885\n",
            "Epoch: 12... Training loss: 0.2383255958557129... Accuracy: 0.8915833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2479 - accuracy: 0.8893\n",
            "Epoch: 13... Training loss: 0.22711433470249176... Accuracy: 0.898\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2487 - accuracy: 0.8901\n",
            "Epoch: 14... Training loss: 0.22263295948505402... Accuracy: 0.9086666666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2456 - accuracy: 0.8886\n",
            "Epoch: 15... Training loss: 0.3075641691684723... Accuracy: 0.8554166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2571 - accuracy: 0.8832\n",
            "Epoch: 16... Training loss: 0.21000678837299347... Accuracy: 0.9108333333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2284 - accuracy: 0.8955\n",
            "Epoch: 17... Training loss: 0.21998831629753113... Accuracy: 0.8975\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2458 - accuracy: 0.8898\n",
            "Epoch: 18... Training loss: 0.2062075138092041... Accuracy: 0.9119166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2374 - accuracy: 0.8914\n",
            "Epoch: 19... Training loss: 0.2449524700641632... Accuracy: 0.8861666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2349 - accuracy: 0.8964\n",
            "Epoch: 20... Training loss: 0.2068728804588318... Accuracy: 0.904\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2200 - accuracy: 0.9033\n",
            "Epoch: 21... Training loss: 0.24976994097232819... Accuracy: 0.88625\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2124 - accuracy: 0.9051\n",
            "Epoch: 22... Training loss: 0.2570517659187317... Accuracy: 0.8853333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2119 - accuracy: 0.9049\n",
            "Epoch: 23... Training loss: 0.24719376862049103... Accuracy: 0.8850833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2231 - accuracy: 0.8980\n",
            "Epoch: 24... Training loss: 0.1937844604253769... Accuracy: 0.9163333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2018 - accuracy: 0.9116\n",
            "Epoch: 25... Training loss: 0.24044939875602722... Accuracy: 0.8921666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2030 - accuracy: 0.9112\n",
            "Epoch: 26... Training loss: 0.2607786953449249... Accuracy: 0.88375\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2227 - accuracy: 0.8996\n",
            "Epoch: 27... Training loss: 0.17065726220607758... Accuracy: 0.9276666666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2023 - accuracy: 0.9118\n",
            "Epoch: 28... Training loss: 0.20031648874282837... Accuracy: 0.9130833333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2123 - accuracy: 0.9059\n",
            "Epoch: 29... Training loss: 0.17264249920845032... Accuracy: 0.9283333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2206 - accuracy: 0.9045\n",
            "Epoch: 30... Training loss: 0.19872289896011353... Accuracy: 0.9090833333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1970 - accuracy: 0.9140\n",
            "Epoch: 31... Training loss: 0.20270879566669464... Accuracy: 0.9185\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1818 - accuracy: 0.9231\n",
            "Epoch: 32... Training loss: 0.19673040509223938... Accuracy: 0.91725\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1931 - accuracy: 0.9162\n",
            "Epoch: 33... Training loss: 0.18557564914226532... Accuracy: 0.9220833333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1809 - accuracy: 0.9205\n",
            "Epoch: 34... Training loss: 0.21139603853225708... Accuracy: 0.9023333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.2150 - accuracy: 0.9071\n",
            "Epoch: 35... Training loss: 0.18266192078590393... Accuracy: 0.9203333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1940 - accuracy: 0.9128\n",
            "Epoch: 36... Training loss: 0.19144316017627716... Accuracy: 0.9175833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1903 - accuracy: 0.9206\n",
            "Epoch: 37... Training loss: 0.22905966639518738... Accuracy: 0.9026666666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1860 - accuracy: 0.9226\n",
            "Epoch: 38... Training loss: 0.19079655408859253... Accuracy: 0.9156666666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.9239\n",
            "Epoch: 39... Training loss: 0.18340356647968292... Accuracy: 0.9195833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1954 - accuracy: 0.9177\n",
            "Epoch: 40... Training loss: 0.17193937301635742... Accuracy: 0.9244166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1861 - accuracy: 0.9182\n",
            "Epoch: 41... Training loss: 0.19515438377857208... Accuracy: 0.9155833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1898 - accuracy: 0.9190\n",
            "Epoch: 42... Training loss: 0.17250479757785797... Accuracy: 0.9285\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1930 - accuracy: 0.9230\n",
            "Epoch: 43... Training loss: 0.15261724591255188... Accuracy: 0.93575\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1698 - accuracy: 0.9300\n",
            "Epoch: 44... Training loss: 0.16871191561222076... Accuracy: 0.9299166666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1759 - accuracy: 0.9244\n",
            "Epoch: 45... Training loss: 0.1702667474746704... Accuracy: 0.9255833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1898 - accuracy: 0.9154\n",
            "Epoch: 46... Training loss: 0.13883031904697418... Accuracy: 0.9453333333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1655 - accuracy: 0.9278\n",
            "Epoch: 47... Training loss: 0.1706106960773468... Accuracy: 0.9295833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1934 - accuracy: 0.9179\n",
            "Epoch: 48... Training loss: 0.19802594184875488... Accuracy: 0.9156666666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1846 - accuracy: 0.9161\n",
            "Epoch: 49... Training loss: 0.2379062920808792... Accuracy: 0.89575\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1961 - accuracy: 0.9137\n",
            "Epoch: 50... Training loss: 0.16812622547149658... Accuracy: 0.9295\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1610 - accuracy: 0.9325\n",
            "Epoch: 51... Training loss: 0.1603277325630188... Accuracy: 0.9304166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1741 - accuracy: 0.9272\n",
            "Epoch: 52... Training loss: 0.15118804574012756... Accuracy: 0.9369166666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1654 - accuracy: 0.9285\n",
            "Epoch: 53... Training loss: 0.15687443315982819... Accuracy: 0.9334166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1879 - accuracy: 0.9217\n",
            "Epoch: 54... Training loss: 0.1515216827392578... Accuracy: 0.9355\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1807 - accuracy: 0.9227\n",
            "Epoch: 55... Training loss: 0.16923411190509796... Accuracy: 0.9275\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1681 - accuracy: 0.9281\n",
            "Epoch: 56... Training loss: 0.14127422869205475... Accuracy: 0.9380833333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1618 - accuracy: 0.9303\n",
            "Epoch: 57... Training loss: 0.1802157461643219... Accuracy: 0.9229166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1692 - accuracy: 0.9271\n",
            "Epoch: 58... Training loss: 0.16164031624794006... Accuracy: 0.9280833333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1597 - accuracy: 0.9313\n",
            "Epoch: 59... Training loss: 0.23996318876743317... Accuracy: 0.9\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1732 - accuracy: 0.9236\n",
            "Epoch: 60... Training loss: 0.1686844825744629... Accuracy: 0.9308333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1673 - accuracy: 0.9255\n",
            "Epoch: 61... Training loss: 0.234028160572052... Accuracy: 0.9008333333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1590 - accuracy: 0.9312\n",
            "Epoch: 62... Training loss: 0.1574738621711731... Accuracy: 0.93225\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1740 - accuracy: 0.9282\n",
            "Epoch: 63... Training loss: 0.16623137891292572... Accuracy: 0.9255\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1721 - accuracy: 0.9268\n",
            "Epoch: 64... Training loss: 0.13799259066581726... Accuracy: 0.9434166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1647 - accuracy: 0.9301\n",
            "Epoch: 65... Training loss: 0.195887953042984... Accuracy: 0.9164166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1702 - accuracy: 0.9262\n",
            "Epoch: 66... Training loss: 0.1545000970363617... Accuracy: 0.9301666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1789 - accuracy: 0.9234\n",
            "Epoch: 67... Training loss: 0.16272957623004913... Accuracy: 0.9323333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1587 - accuracy: 0.9314\n",
            "Epoch: 68... Training loss: 0.14739017188549042... Accuracy: 0.9338333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1523 - accuracy: 0.9356\n",
            "Epoch: 69... Training loss: 0.1312481313943863... Accuracy: 0.9444166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1626 - accuracy: 0.9297\n",
            "Epoch: 70... Training loss: 0.1555284559726715... Accuracy: 0.9366666666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1579 - accuracy: 0.9322\n",
            "Epoch: 71... Training loss: 0.15133841335773468... Accuracy: 0.935\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1394 - accuracy: 0.9420\n",
            "Epoch: 72... Training loss: 0.21766981482505798... Accuracy: 0.9083333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1683 - accuracy: 0.9288\n",
            "Epoch: 73... Training loss: 0.14324218034744263... Accuracy: 0.94\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1498 - accuracy: 0.9346\n",
            "Epoch: 74... Training loss: 0.15899468958377838... Accuracy: 0.92825\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1710 - accuracy: 0.9268\n",
            "Epoch: 75... Training loss: 0.16236397624015808... Accuracy: 0.9335833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1637 - accuracy: 0.9298\n",
            "Epoch: 76... Training loss: 0.15067370235919952... Accuracy: 0.93375\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1497 - accuracy: 0.9387\n",
            "Epoch: 77... Training loss: 0.14019887149333954... Accuracy: 0.94175\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1581 - accuracy: 0.9296\n",
            "Epoch: 78... Training loss: 0.14237622916698456... Accuracy: 0.9353333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1553 - accuracy: 0.9342\n",
            "Epoch: 79... Training loss: 0.13143424689769745... Accuracy: 0.943\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1570 - accuracy: 0.9346\n",
            "Epoch: 80... Training loss: 0.12584853172302246... Accuracy: 0.9476666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1496 - accuracy: 0.9357\n",
            "Epoch: 81... Training loss: 0.13699007034301758... Accuracy: 0.9410833333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1619 - accuracy: 0.9303\n",
            "Epoch: 82... Training loss: 0.137881800532341... Accuracy: 0.94\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1556 - accuracy: 0.9325\n",
            "Epoch: 83... Training loss: 0.19284950196743011... Accuracy: 0.9158333333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1493 - accuracy: 0.9369\n",
            "Epoch: 84... Training loss: 0.178207129240036... Accuracy: 0.9238333333333333\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1380 - accuracy: 0.9421\n",
            "Epoch: 85... Training loss: 0.17846907675266266... Accuracy: 0.9228333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1590 - accuracy: 0.9318\n",
            "Epoch: 86... Training loss: 0.1189296692609787... Accuracy: 0.95125\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1489 - accuracy: 0.9356\n",
            "Epoch: 87... Training loss: 0.1674850881099701... Accuracy: 0.9284166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1383 - accuracy: 0.9395\n",
            "Epoch: 88... Training loss: 0.141178697347641... Accuracy: 0.939\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1398 - accuracy: 0.9408\n",
            "Epoch: 89... Training loss: 0.12807908654212952... Accuracy: 0.9469166666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1499 - accuracy: 0.9380\n",
            "Epoch: 90... Training loss: 0.16912490129470825... Accuracy: 0.9261666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1542 - accuracy: 0.9374\n",
            "Epoch: 91... Training loss: 0.18079020082950592... Accuracy: 0.92075\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1474 - accuracy: 0.9376\n",
            "Epoch: 92... Training loss: 0.13960866630077362... Accuracy: 0.9408333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1490 - accuracy: 0.9356\n",
            "Epoch: 93... Training loss: 0.12486740946769714... Accuracy: 0.94725\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1439 - accuracy: 0.9364\n",
            "Epoch: 94... Training loss: 0.15524746477603912... Accuracy: 0.9336666666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1652 - accuracy: 0.9320\n",
            "Epoch: 95... Training loss: 0.16892288625240326... Accuracy: 0.926\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1511 - accuracy: 0.9333\n",
            "Epoch: 96... Training loss: 0.14704295992851257... Accuracy: 0.937\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1566 - accuracy: 0.9320\n",
            "Epoch: 97... Training loss: 0.18370261788368225... Accuracy: 0.91575\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1565 - accuracy: 0.9336\n",
            "Epoch: 98... Training loss: 0.14090123772621155... Accuracy: 0.93875\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1389 - accuracy: 0.9393\n",
            "Epoch: 99... Training loss: 0.1612466424703598... Accuracy: 0.9315833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1441 - accuracy: 0.9364\n",
            "Epoch: 100... Training loss: 0.13333837687969208... Accuracy: 0.9420833333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1492 - accuracy: 0.9378\n",
            "Epoch: 101... Training loss: 0.12715917825698853... Accuracy: 0.9459166666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1398 - accuracy: 0.9400\n",
            "Epoch: 102... Training loss: 0.1669939011335373... Accuracy: 0.9279166666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1682 - accuracy: 0.9285\n",
            "Epoch: 103... Training loss: 0.1183377057313919... Accuracy: 0.9521666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1604 - accuracy: 0.9296\n",
            "Epoch: 104... Training loss: 0.1217799037694931... Accuracy: 0.9475\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1403 - accuracy: 0.9401\n",
            "Epoch: 105... Training loss: 0.16276520490646362... Accuracy: 0.925\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1447 - accuracy: 0.9354\n",
            "Epoch: 106... Training loss: 0.16108453273773193... Accuracy: 0.9295833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1614 - accuracy: 0.9353\n",
            "Epoch: 107... Training loss: 0.12508656084537506... Accuracy: 0.9455833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1458 - accuracy: 0.9388\n",
            "Epoch: 108... Training loss: 0.1150483712553978... Accuracy: 0.9520833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1369 - accuracy: 0.9409\n",
            "Epoch: 109... Training loss: 0.11704057455062866... Accuracy: 0.9495\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1367 - accuracy: 0.9417\n",
            "Epoch: 110... Training loss: 0.14086739718914032... Accuracy: 0.9366666666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1563 - accuracy: 0.9340\n",
            "Epoch: 111... Training loss: 0.20253194868564606... Accuracy: 0.9134166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1594 - accuracy: 0.9288\n",
            "Epoch: 112... Training loss: 0.18702539801597595... Accuracy: 0.9235\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1519 - accuracy: 0.9340\n",
            "Epoch: 113... Training loss: 0.13154037296772003... Accuracy: 0.9428333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1276 - accuracy: 0.9468\n",
            "Epoch: 114... Training loss: 0.1562941074371338... Accuracy: 0.932\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1389 - accuracy: 0.9416\n",
            "Epoch: 115... Training loss: 0.1455315202474594... Accuracy: 0.9379166666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1323 - accuracy: 0.9430\n",
            "Epoch: 116... Training loss: 0.14434267580509186... Accuracy: 0.9348333333333333\n",
            "375/375 [==============================] - 2s 4ms/step - loss: 0.1501 - accuracy: 0.9376\n",
            "Epoch: 117... Training loss: 0.16657598316669464... Accuracy: 0.9240833333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1345 - accuracy: 0.9413\n",
            "Epoch: 118... Training loss: 0.11704181134700775... Accuracy: 0.95175\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1273 - accuracy: 0.9417\n",
            "Epoch: 119... Training loss: 0.12680092453956604... Accuracy: 0.9445\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1376 - accuracy: 0.9426\n",
            "Epoch: 120... Training loss: 0.1337442845106125... Accuracy: 0.93975\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1373 - accuracy: 0.9419\n",
            "Epoch: 121... Training loss: 0.17282389104366302... Accuracy: 0.9233333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1334 - accuracy: 0.9419\n",
            "Epoch: 122... Training loss: 0.13748331367969513... Accuracy: 0.9415833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1331 - accuracy: 0.9440\n",
            "Epoch: 123... Training loss: 0.13161645829677582... Accuracy: 0.9455\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1383 - accuracy: 0.9419\n",
            "Epoch: 124... Training loss: 0.1471380889415741... Accuracy: 0.9355833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1506 - accuracy: 0.9381\n",
            "Epoch: 125... Training loss: 0.1353466510772705... Accuracy: 0.94175\n",
            "375/375 [==============================] - 2s 2ms/step - loss: 0.1504 - accuracy: 0.9376\n",
            "Epoch: 126... Training loss: 0.13010942935943604... Accuracy: 0.9464166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1313 - accuracy: 0.9477\n",
            "Epoch: 127... Training loss: 0.12851765751838684... Accuracy: 0.9463333333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1490 - accuracy: 0.9396\n",
            "Epoch: 128... Training loss: 0.1287088245153427... Accuracy: 0.9425833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1343 - accuracy: 0.9415\n",
            "Epoch: 129... Training loss: 0.15586479008197784... Accuracy: 0.9314166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1406 - accuracy: 0.9394\n",
            "Epoch: 130... Training loss: 0.11502083390951157... Accuracy: 0.95425\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1376 - accuracy: 0.9404\n",
            "Epoch: 131... Training loss: 0.13091742992401123... Accuracy: 0.9411666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1411 - accuracy: 0.9378\n",
            "Epoch: 132... Training loss: 0.1318255215883255... Accuracy: 0.94225\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1339 - accuracy: 0.9404\n",
            "Epoch: 133... Training loss: 0.11071262508630753... Accuracy: 0.9531666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1407 - accuracy: 0.9408\n",
            "Epoch: 134... Training loss: 0.15994754433631897... Accuracy: 0.9294166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1467 - accuracy: 0.9387\n",
            "Epoch: 135... Training loss: 0.11068151891231537... Accuracy: 0.9535833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1388 - accuracy: 0.9384\n",
            "Epoch: 136... Training loss: 0.1315898895263672... Accuracy: 0.9445\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1220 - accuracy: 0.9449\n",
            "Epoch: 137... Training loss: 0.1650468409061432... Accuracy: 0.9275\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1416 - accuracy: 0.9365\n",
            "Epoch: 138... Training loss: 0.12210424989461899... Accuracy: 0.9475\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1377 - accuracy: 0.9393\n",
            "Epoch: 139... Training loss: 0.12153225392103195... Accuracy: 0.9488333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1386 - accuracy: 0.9397\n",
            "Epoch: 140... Training loss: 0.21967890858650208... Accuracy: 0.911\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1695 - accuracy: 0.9265\n",
            "Epoch: 141... Training loss: 0.14073945581912994... Accuracy: 0.9378333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1312 - accuracy: 0.9444\n",
            "Epoch: 142... Training loss: 0.11731408536434174... Accuracy: 0.9489166666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1315 - accuracy: 0.9448\n",
            "Epoch: 143... Training loss: 0.12784694135189056... Accuracy: 0.945\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1408 - accuracy: 0.9392\n",
            "Epoch: 144... Training loss: 0.1268192082643509... Accuracy: 0.9484166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1361 - accuracy: 0.9417\n",
            "Epoch: 145... Training loss: 0.13952791690826416... Accuracy: 0.9365833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1364 - accuracy: 0.9406\n",
            "Epoch: 146... Training loss: 0.13423222303390503... Accuracy: 0.9405\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1266 - accuracy: 0.9493\n",
            "Epoch: 147... Training loss: 0.23273786902427673... Accuracy: 0.9050833333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1473 - accuracy: 0.9389\n",
            "Epoch: 148... Training loss: 0.11738945543766022... Accuracy: 0.9491666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1409 - accuracy: 0.9388\n",
            "Epoch: 149... Training loss: 0.10619380325078964... Accuracy: 0.9561666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1431 - accuracy: 0.9394\n",
            "Epoch: 150... Training loss: 0.1362709403038025... Accuracy: 0.94575\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1444 - accuracy: 0.9365\n",
            "Epoch: 151... Training loss: 0.12094063311815262... Accuracy: 0.9485833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1336 - accuracy: 0.9419\n",
            "Epoch: 152... Training loss: 0.12824489176273346... Accuracy: 0.9453333333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1348 - accuracy: 0.9413\n",
            "Epoch: 153... Training loss: 0.14719133079051971... Accuracy: 0.9331666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1298 - accuracy: 0.9455\n",
            "Epoch: 154... Training loss: 0.1398448497056961... Accuracy: 0.9376666666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1306 - accuracy: 0.9428\n",
            "Epoch: 155... Training loss: 0.18590161204338074... Accuracy: 0.9238333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1343 - accuracy: 0.9411\n",
            "Epoch: 156... Training loss: 0.2517450451850891... Accuracy: 0.902\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1563 - accuracy: 0.9330\n",
            "Epoch: 157... Training loss: 0.13553740084171295... Accuracy: 0.9395833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1477 - accuracy: 0.9375\n",
            "Epoch: 158... Training loss: 0.14372946321964264... Accuracy: 0.93825\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1277 - accuracy: 0.9443\n",
            "Epoch: 159... Training loss: 0.16107740998268127... Accuracy: 0.9288333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1352 - accuracy: 0.9401\n",
            "Epoch: 160... Training loss: 0.13048042356967926... Accuracy: 0.9420833333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1288 - accuracy: 0.9447\n",
            "Epoch: 161... Training loss: 0.10897824913263321... Accuracy: 0.9535833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1373 - accuracy: 0.9370\n",
            "Epoch: 162... Training loss: 0.11060400307178497... Accuracy: 0.9559166666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1421 - accuracy: 0.9389\n",
            "Epoch: 163... Training loss: 0.11912135779857635... Accuracy: 0.947\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1301 - accuracy: 0.9441\n",
            "Epoch: 164... Training loss: 0.19621075689792633... Accuracy: 0.9165\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1368 - accuracy: 0.9393\n",
            "Epoch: 165... Training loss: 0.1083652526140213... Accuracy: 0.9551666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1292 - accuracy: 0.9433\n",
            "Epoch: 166... Training loss: 0.12260996550321579... Accuracy: 0.94725\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1344 - accuracy: 0.9420\n",
            "Epoch: 167... Training loss: 0.18252238631248474... Accuracy: 0.922\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1298 - accuracy: 0.9440\n",
            "Epoch: 168... Training loss: 0.12331950664520264... Accuracy: 0.94575\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1392 - accuracy: 0.9401\n",
            "Epoch: 169... Training loss: 0.11962275952100754... Accuracy: 0.94525\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1432 - accuracy: 0.9363\n",
            "Epoch: 170... Training loss: 0.11473891139030457... Accuracy: 0.9509166666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1294 - accuracy: 0.9430\n",
            "Epoch: 171... Training loss: 0.1369313895702362... Accuracy: 0.9415\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1289 - accuracy: 0.9458\n",
            "Epoch: 172... Training loss: 0.11227001994848251... Accuracy: 0.9509166666666666\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.1255 - accuracy: 0.9486\n",
            "Epoch: 173... Training loss: 0.11723577231168747... Accuracy: 0.94925\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1373 - accuracy: 0.9382\n",
            "Epoch: 174... Training loss: 0.11156494915485382... Accuracy: 0.9523333333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1271 - accuracy: 0.9431\n",
            "Epoch: 175... Training loss: 0.10746780037879944... Accuracy: 0.9549166666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1236 - accuracy: 0.9479\n",
            "Epoch: 176... Training loss: 0.16553384065628052... Accuracy: 0.92375\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1385 - accuracy: 0.9408\n",
            "Epoch: 177... Training loss: 0.15914760529994965... Accuracy: 0.92875\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1460 - accuracy: 0.9366\n",
            "Epoch: 178... Training loss: 0.1310354769229889... Accuracy: 0.9445\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1187 - accuracy: 0.9502\n",
            "Epoch: 179... Training loss: 0.11555583029985428... Accuracy: 0.94925\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1320 - accuracy: 0.9391\n",
            "Epoch: 180... Training loss: 0.10295811295509338... Accuracy: 0.9573333333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1326 - accuracy: 0.9424\n",
            "Epoch: 181... Training loss: 0.11737953871488571... Accuracy: 0.9481666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1186 - accuracy: 0.9497\n",
            "Epoch: 182... Training loss: 0.19007115066051483... Accuracy: 0.9165\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1372 - accuracy: 0.9404\n",
            "Epoch: 183... Training loss: 0.14276152849197388... Accuracy: 0.93825\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1346 - accuracy: 0.9444\n",
            "Epoch: 184... Training loss: 0.10345998406410217... Accuracy: 0.95875\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1319 - accuracy: 0.9443\n",
            "Epoch: 185... Training loss: 0.11196659505367279... Accuracy: 0.9518333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1344 - accuracy: 0.9388\n",
            "Epoch: 186... Training loss: 0.1532173603773117... Accuracy: 0.9324166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1371 - accuracy: 0.9401\n",
            "Epoch: 187... Training loss: 0.13315580785274506... Accuracy: 0.94175\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1274 - accuracy: 0.9461\n",
            "Epoch: 188... Training loss: 0.21884813904762268... Accuracy: 0.9096666666666666\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1378 - accuracy: 0.9422\n",
            "Epoch: 189... Training loss: 0.11005853116512299... Accuracy: 0.9535833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9474\n",
            "Epoch: 190... Training loss: 0.144888773560524... Accuracy: 0.93725\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1228 - accuracy: 0.9454\n",
            "Epoch: 191... Training loss: 0.1208001896739006... Accuracy: 0.9455833333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1365 - accuracy: 0.9409\n",
            "Epoch: 192... Training loss: 0.125482439994812... Accuracy: 0.9461666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1235 - accuracy: 0.9478\n",
            "Epoch: 193... Training loss: 0.1606072038412094... Accuracy: 0.9314166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1282 - accuracy: 0.9431\n",
            "Epoch: 194... Training loss: 0.17626598477363586... Accuracy: 0.9238333333333333\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1264 - accuracy: 0.9427\n",
            "Epoch: 195... Training loss: 0.12441151589155197... Accuracy: 0.9434166666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1314 - accuracy: 0.9442\n",
            "Epoch: 196... Training loss: 0.13632400333881378... Accuracy: 0.9426666666666667\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1290 - accuracy: 0.9437\n",
            "Epoch: 197... Training loss: 0.18325260281562805... Accuracy: 0.92075\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1288 - accuracy: 0.9439\n",
            "Epoch: 198... Training loss: 0.139119490981102... Accuracy: 0.9380833333333334\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.1355 - accuracy: 0.9421\n",
            "Epoch: 199... Training loss: 0.1738370954990387... Accuracy: 0.92575\n",
            "Confusion matrix:\n",
            "[[1467   20    0]\n",
            " [  79 1019   10]\n",
            " [   0  102  303]]\n",
            "Accuracy: \n",
            "0.9296666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywAI6WfHujxe"
      },
      "source": [
        "### MNIST Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15_FXzMGVK1j"
      },
      "source": [
        "def tf_mnist_classification():\n",
        "    # Load data from file\n",
        "    # Make sure that fashion-mnist/*.gz is in data/\n",
        "    train_X, train_Y, val_X, val_Y, test_X, test_Y = get_mnist_data(1)\n",
        "    train_X, val_X, test_X = normalize(train_X, val_X, test_X)    \n",
        "    \n",
        "    num_class = (np.unique(train_Y)).shape[0]\n",
        "\n",
        "    # Pad 1 as the third feature of train_x and test_x\n",
        "    train_X = add_one(train_X)\n",
        "    val_X = add_one(val_X)\n",
        "    test_X = add_one(test_X)\n",
        "    \n",
        "    train_Y = create_one_hot(train_Y, num_class)\n",
        "    val_Y = create_one_hot(val_Y, num_class)\n",
        "\n",
        "    # Define hyper-parameters and train-related parameters\n",
        "    hidden_layers = [128, 256, 100, 64]\n",
        "    learning_rate = 0.005\n",
        "    epochs = 20\n",
        "    activation = tf.nn.relu\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    #Initialize model\n",
        "    dnn = DNNModel(hidden_layers=hidden_layers, num_classes=num_class,\n",
        "                   activation=activation, epochs=epochs, optimizer=optimizer)\n",
        "    #Train\n",
        "    dnn.train(train_X, train_Y)\n",
        "\n",
        "    # TEST\n",
        "    # Confusion matrix\n",
        "    metrics = confusion_matrix(test_Y, dnn.predict(test_X))\n",
        "    print('Confusion matrix:')\n",
        "    print(metrics)\n",
        "    print(\"Accuracy: \")\n",
        "    print(metrics.trace()/test_Y.shape[0])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv9L55P8VK1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5351187-6ed5-47bd-8cfc-054328750f10"
      },
      "source": [
        "tf_mnist_classification()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading fashion MNIST data...\n",
            "Done reading\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6819 - accuracy: 0.7592\n",
            "Epoch: 0... Training loss: 0.4267736077308655... Accuracy: 0.84146\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4366 - accuracy: 0.8452\n",
            "Epoch: 1... Training loss: 0.41568389534950256... Accuracy: 0.85606\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4174 - accuracy: 0.8514\n",
            "Epoch: 2... Training loss: 0.37317684292793274... Accuracy: 0.86644\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4029 - accuracy: 0.8587\n",
            "Epoch: 3... Training loss: 0.37526729702949524... Accuracy: 0.8716\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3808 - accuracy: 0.8683\n",
            "Epoch: 4... Training loss: 0.41083475947380066... Accuracy: 0.85528\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3843 - accuracy: 0.8667\n",
            "Epoch: 5... Training loss: 0.3668743669986725... Accuracy: 0.8762\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3660 - accuracy: 0.8751\n",
            "Epoch: 6... Training loss: 0.3487933576107025... Accuracy: 0.87484\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.3604 - accuracy: 0.8734\n",
            "Epoch: 7... Training loss: 0.34173592925071716... Accuracy: 0.88052\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.3579 - accuracy: 0.8772\n",
            "Epoch: 8... Training loss: 0.31953591108322144... Accuracy: 0.88674\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3532 - accuracy: 0.8799\n",
            "Epoch: 9... Training loss: 0.33932921290397644... Accuracy: 0.88182\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3519 - accuracy: 0.8790\n",
            "Epoch: 10... Training loss: 0.34378889203071594... Accuracy: 0.88678\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.3401 - accuracy: 0.8826\n",
            "Epoch: 11... Training loss: 0.3150520622730255... Accuracy: 0.88814\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.3343 - accuracy: 0.8814\n",
            "Epoch: 12... Training loss: 0.5123496055603027... Accuracy: 0.82732\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.3838 - accuracy: 0.8709\n",
            "Epoch: 13... Training loss: 0.3316517174243927... Accuracy: 0.88416\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3443 - accuracy: 0.8803\n",
            "Epoch: 14... Training loss: 0.3290870785713196... Accuracy: 0.89126\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3353 - accuracy: 0.8844\n",
            "Epoch: 15... Training loss: 0.33826422691345215... Accuracy: 0.88164\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3597 - accuracy: 0.8810\n",
            "Epoch: 16... Training loss: 0.3034585118293762... Accuracy: 0.89556\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3103 - accuracy: 0.8905\n",
            "Epoch: 17... Training loss: 0.3113674521446228... Accuracy: 0.8909\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3499 - accuracy: 0.8832\n",
            "Epoch: 18... Training loss: 0.35384365916252136... Accuracy: 0.87936\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.3410 - accuracy: 0.8852\n",
            "Epoch: 19... Training loss: 0.28530266880989075... Accuracy: 0.8998\n",
            "Confusion matrix:\n",
            "[[793   1  13  32   1   0 143   0  17   0]\n",
            " [  7 955   1  29   3   0   4   0   1   0]\n",
            " [  9   0 743  14 110   0 117   0   7   0]\n",
            " [ 26   2  13 897  24   0  33   0   5   0]\n",
            " [  1   0  99  46 790   0  58   0   6   0]\n",
            " [  0   0   0   1   0 952   0  24   2  21]\n",
            " [110   1  66  24  74   0 703   0  22   0]\n",
            " [  0   0   0   0   0  29   0 944   0  27]\n",
            " [  1   0   5   4   2   2   8   7 971   0]\n",
            " [  0   0   0   0   0   4   0  38   1 957]]\n",
            "Accuracy: \n",
            "0.8705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2V4EFbK6qqR"
      },
      "source": [
        "# III. Nộp bài\n",
        "Sau khi thực hiện xong các bạn cần upload file bài làm .ipynb theo hướng dẫn ở [form này](https://forms.gle/yUdsDcYV6Z5eFFr89)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A796xi6AaXwy"
      },
      "source": [
        "## IV. Thang điểm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojErsXU5xX6u"
      },
      "source": [
        "| TODO  | Điểm  |   |   |   |\n",
        "|---|---|---|---|---|\n",
        "|\\[TODO 1\\] Cài đặt các hàm activation   |  1 |   |   |   |\n",
        "|\\[TODO 2\\] Hàm `forward`/`HiddenLayer` | 1  |   |   |   |\n",
        "|\\[TODO 3\\] Hàm `backward`/`HiddenLayer`  | 2  |   |   |   |\n",
        "|\\[TODO 4\\] Hàm `forward`/`NeuralNetwork`  | 0.5  |   |   |   |\n",
        "|\\[TODO 5\\] Hàm `compute_loss`/`NeuralNetwork`  | 1.5  |   |   |   |\n",
        "|\\[TODO 6\\] Hàm `compute_delta_grad_last`/`NeuralNetwork`  | 1  |   |   |   |\n",
        "|\\[TODO 7\\] Hàm `backward`/`NeuralNetwork`  | 1  |   |   |   |\n",
        "|\\[TODO 8\\] Hàm `update_weight_momentum`/`NeuralNetwork`  | 1  |   |   |   |\n",
        "|\\[TODO 9\\] Hàm `minibatch_train`  | 2  |   |   |   |\n",
        "|\\[TODO 10\\] Cài đặt kiến trúc Neural network sử dụng tensorflow  | 1  |   |   |   |\n",
        "|\\[TODO 11\\] Cài đặt hàm tính accuracy  | 1  |   |   |   |\n",
        "|\\[TODO 12\\] Cài đặt hàm train trong class `DNNModel`  | 2  |   |   |   |\n",
        "|**Tổng**| **15**  |   |   |   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWgbEVveBxji"
      },
      "source": [
        "## Author: Giang Tran, Hoa Nguyen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnHV18q7eVgA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}